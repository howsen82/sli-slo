# SLIs and SLOs at the Heart of Reliability

Before developing and applying **Service Level Objectives** (**SLOs**), it is essential to establish a shared understanding among all stakeholders of the components being measured and of what defines success. This chapter briefly introduces reliability engineering at a broad level, emphasizing the relationship between **Service Level Indicators** (**SLIs**) and SLOs, both within reliability engineering and as fundamental concepts. The chapter will outline the foundational principles of reliability engineering and guide you toward initiating workshops or discussions to gather the requirements and identify the necessary metrics to measure.

Each metric you identify for your application or platform will play a key role in determining its respective availability, performance, and reliability. We will guide you through examining various practices, principles, and implementations of SLIs and SLOs. This journey will cover concepts such as their definition, measurement, hierarchy, and relationship to observability and monitoring, as well as the consequences of not choosing the appropriate indicators.

Consider this chapter as a refresher for the practice of reliability engineering, as well as the definitions of core concepts and bringing the dependencies between each core concept to the forefront. This will ensure that before running your workshop, all participants have a clear understanding of the practice itself and the best practices surrounding structuring the workshop, and thus the team(s) for success.

In this chapter, we’ll cover the following main topics:

* Reliability engineering principles and practices
* The hierarchy of reliability engineering
* The relationship between observability and monitoring
* The cost of not choosing SLIs

Let’s get started!

# Reliability engineering principles and practices

In today’s technology industry, various standard and non-standard practices within sub-industries make up the core functions and processes within each organization. This can often lead to the development of several disciplines consisting of various opinions and different ways of doing things – that is, until a standard is created and unification happens. Further development then occurs from a cycle of implementation and formulating some hypotheses and strong opinions on how things should be done.

This is not said to shed a negative light but to emphasize how disciplines, despite their industry, begin with starting and vocalizing opinions. This is true whether people agree or disagree with them. Therefore, before diving into the complexities of reliability engineering, it’s important to understand the various pillars, roles, responsibilities, and frameworks that create the practice as we know it today. From software development to infrastructure in distributed environments, and from storage networking to cybersecurity, each technical space presents its unique challenges and opportunities with performance tracking. By acknowledging this diversity within technology, organizations can better appreciate continuing to improve upon the breadth and depth of reliability engineering’s applicability.

## The importance of reliability in modern systems and services

In today’s technical industry, there is an ongoing adoption of software, infrastructure, and platforms as services. This creates expectations of availability, high performance, and minimal latency, among other requirements. Along with provided services is a level of commitment that is made to external consumers. Within most organizations, you will see implemented **Service Level Agreements** (**SLAs**), which inform the service user of the level or type of performance they can expect from your service, as well as how matters are handled in the event this commitment is not met.

SLIs and SLOs serve as linchpins in the pursuit of reliability excellence. SLIs are quantifiable metrics that represent the performance and health of systems, providing tangible measurements of reliability. SLOs, on the other hand, establish acceptable thresholds or targets for SLIs, aligning technical objectives with business requirements. The relationship between SLIs and SLOs is symbiotic, with SLOs guiding the establishment of meaningful SLIs and serving as the benchmark against which system performance is measured. Together, they form the foundation of a proactive and data-driven approach to reliability engineering.

## Introduction to key reliability concepts

Reliability engineering is a multifaceted discipline that incorporates a range of methodologies, tools, and practices to ensure systems operate consistently and predictably under different conditions. At its core, reliability engineering seeks to minimize the occurrence and impact of failures, thereby enhancing system reliability and availability.

Key concepts as they relate to reducing failures within the scope of reliability engineering include topics such as the following:

* **Service level quantifiers**: SLIs, SLOs, and SLAs; the core topic of this book.
* **Error budgets**: A measurement of the allowable amount of downtime or outage of some component within the system.
* **Fault tolerance**: The ability of a system to operate without failure when a single or multiple components fail.
* **Redundancy**: The duplication of systems or components as a backup mechanism.
* **Failure analysis**: The process of collecting and analyzing data to determine the cause of a failure.
* **Incident response**: The set of processes in place for responding to and managing incidents.
* **Capacity planning**: The set practices in place to help determine the current and future demand(s).
* **Monitoring**: The set of processes and tooling used to monitor, and later alert, events within the system.

Each item in the list is central to the discipline, providing the foundation upon which reliability strategies are built. The proper implementation of each component enhances your services, platforms, and overall customer experience while structuring and shaping how you implement each of the pillars within your reliability engineering organization. We will discuss this topic via the pillars within the hierarchy of reliability engineering in a later section.

## Reliability engineers ensure system reliability and resiliency

With the constant evolution of complexity and interconnection of software and technology, the role of reliability engineers has emerged as an indispensable part of ensuring the resiliency of critical infrastructure. This section focuses on the multifaceted responsibilities and key contributions of reliability engineers in safeguarding systems, while highlighting their role as architects of stability and champions of operational excellence. Within the industry, you will often hear about **Site Reliability Engineers** (**SREs**), who are paired with DevOps engineers and occasionally compared to systems engineers. However, understanding the SREs’ role is crucial to assigning them to the right initiatives that align with their specialties, and to clearly define collaboration within an organization for unified and efficient teamwork.

Reliability engineers are tasked with the following responsibilities:

* Designing, implementing, and maintaining systems and processes that meet stringent reliability requirements.
* SREs play a crucial role in ensuring that systems operate consistently and predictably under varying conditions, minimizing the occurrence and impact of failures; this can relate to enhancing processes through automation tooling.
* Reliability engineers leverage a combination of technical expertise, data-driven methodologies, and cross-functional collaboration to drive continuous improvement and enhance system resilience.

The various responsibilities of an SRE will often require them to maintain a broader scope of visibility within the organization. Thus, their responsibilities will sometimes span a wide range of activities aimed at promoting system reliability, as opposed to building it, to increase buy-in. They design and implement monitoring systems and alerting mechanisms to detect and address anomalies in real time. This will include conducting sessions such as retrospective analysis of incidents to identify root causes and implement preventive measures. Additionally, reliability engineers collaborate with cross-functional teams to design and implement reliability features and improvements, such as fault tolerance, redundancy, and disaster recovery, as mentioned earlier in this chapter.

Reliability engineers also make significant contributions to the organization through designing and implementing robust monitoring and alerting systems, which enable the proactive identification and resolution of issues before they impact users. Through continuous analysis and optimization of SLIs and SLOs, they can drive improvements in system performance and availability. By conducting thorough post-incident reviews, they identify systemic weaknesses and implement corrective measures to prevent future occurrences. Ultimately, reliability engineers are critical to fostering a culture of reliability excellence through software within the organization, where reliability is prioritized as a core value and marketed as a competitive advantage.

### Challenges and opportunities of the reliability engineering role

Despite the critical importance of their role, reliability engineers face a myriad of challenges. Those include not only determining system reliability but sometimes also aligning their role and the work they do with the broader organization’s goal. These challenges can include the following:

* Managing the complexity of modern systems as requirements change.
* Balancing reliability requirements with other competing priorities.
* Fostering collaboration across diverse teams and stakeholders.

These challenges also present opportunities for growth and innovation through service-level indicators and objectives. By embracing their responsibilities and rising to the challenges of modern technology, reliability engineers can use their technical skill sets to drive positive outcomes and foster a culture of reliability excellence within their organizations. Let’s shift toward understanding the hierarchy of individual pillars that shape the practice of reliability to better understand the interworking and dependencies of each.

# The hierarchy of reliability engineering

The hierarchy of reliability engineering provides the framework for which reliability goals are defined, implemented, and monitored across various layers. From infrastructure and application to business layers, each tier plays a crucial role in ensuring the overall reliability and resilience of systems and services. This section highlights the hierarchical structure of reliability engineering, delineating the responsibilities and objectives of different teams and stakeholders within each layer.

The term *hierarchy* was coined by *Mikey Dickerson*, a Google Engineer, to display the relationship between pillars within the practice. The hierarchy of SRE is based on Maslow’s *Hierarchy of Needs framework*. Maslow’s Hierarchy of Needs model categorizes human needs into two types, comprising various levels of necessity. The hierarchy of SRE functions in a similar manner. Within the practice of SRE, there are several “needs” that are required in a structured order, which you build upon to create a successful practice.

The hierarchy of reliability engineering lists the pillars of reliability engineering in the following order:

* **Product**: The product layer, as it relates to the hierarchy, represents the end goal, or packaged solution, that ships to the customers. It is the result of appropriately implementing each of the pillars.
* **Development**: Reaching the goal of a shippable product requires a seamless development process that includes the identification of a market need, researching industry competition, ideating the solution, developing a product roadmap, and building a **minimum viable** **product** (**MVP**).
* **Capacity planning**: To run an efficient product and produce shippable products, we need to assess internal pipelines to determine the right amount of technical and human resources required to work sustainably.
* **Testing and release procedures**: Central to following up on the action items from retrospectives and root cause analysis is the process of implementing the appropriate test, test frameworks, and release procedures to ship the product most reliably to the target consumer.
* **Retrospectives and root cause analysis**: Retrospectives and root cause analysis are central to assessing and communicating what happened, where it happened, why it happened, and how to mitigate the occurrence moving forward, between the knowledgeable engineers and other staff. We will discuss this topic in more depth in later chapters.
* **Incident response**: This consists of the collective set of practices and tooling utilized to respond to critical events that occur in relation to your application, service, or platform.
* **Monitoring**: This consists of the set of processes and the tooling used to capture the events that occur within your application, service, or platform environment. Quality monitoring is important to better understand what’s happening within your system, where it’s happening, and why. As with most things in the field of technology, shifts, changes, and improvements happen.

During the process of implementing SLIs and SLOs, it was discovered that the hierarchy is not complete if the following conditions are met:

* Not ensuring that we are monitoring the right things.
* Lacking the visibility to ensure that we are monitoring the right things.
* Unclear regarding the boundaries within our systems that integrate to provide a unified piece of functionality to our end users.

The limitations previously mentioned result in the idea of observability serving as the foundation pillar in the hierarchy of reliability engineering. Observability provides us with the ability to monitor the “unknown” and “unknowns” utilizing what is considered the three pillars of observability: *metrics*, *traces*, and *logs*. The goal of this section is not to introduce or define observability but to paint a picture of its importance in the practice and at the foundation of the reliability engineering pillars. We can then view our updated hierarchy to appear as follows:

![Figure 1.1 – The hierarchy of reliability engineering, which includes observability as the foundational building block](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_01_1.jpg)

Figure 1.1 – The hierarchy of reliability engineering, which includes observability as the foundational building block

*Figure 1**.1* represents the updated hierarchy of reliability engineering, which includes the foundational layer of observability. You can also refer to Dickerson’s original *Hierarchy of Reliability Engineering* and further information on it in *Part III* of *Site Reliability Engineering* (Google, Inc., 2017).

The next question you might have is, “What is the difference between observability and monitoring?”. It happens often that the two are mentioned together, although they are not the same. Observability helps us monitor the known unknowns we are aware of, especially when considering aggregating data, while monitoring helps us identify the unknown unknowns. In simpler terms, one helps us monitor a problem we are aware of while the other helps us monitor and identify problems we are not aware of. We will investigate the two a bit more deeply in later chapters.

## Responsibilities and objectives within each pillar

Within each pillar, various teams and stakeholders have distinct responsibilities and objectives geared toward ensuring reliability and resilience. The overarching goal of running successful workshops and thus ensuring successful implementation of your service level quantifiers aligns with your Reliability Engineering organization target goals. Those are as follows:

* Identify the overarching objectives of each pillar.
* Understand the required roles and responsibilities to accurately assign objectives to respective owners.
* Tie in the processes and overlaps for work within each pillar of another pillar. For incidents, retrospectives and incident response are separate pillars but contain several dependencies between them.

Although it’s not the highlight of this chapter or book, it’s important to understand the correlation between successful implementation and how aligned and structured the reliability engineering is within your organization. Even more important is how we communicate this information.

Effective communication and collaboration across layers are both essential for ensuring that reliability goals are aligned and achieved. This includes regular meetings, status updates, and progress reports shared between teams and stakeholders at different levels. Additionally, collaboration tools such as project management software, issue trackers, and documentation repositories can facilitate information sharing and coordination across teams. By fostering a culture of open communication and collaboration, organizations can break down silos and ensure that everyone is working toward common reliability objectives. As much as we may not want to admit it, how we communicate things and the amount of ease we provide regarding communication drastically affects the ability to consume information. This is an important soft skill for SREs working at various levels of an organization, as well as translating information to the target audience across several layers of the management pipeline.

## Aligning reliability goals and objectives across the hierarchy

Alongside successfully communicating and collaborating at various levels of the management pipeline, being able to align reliability goals and objectives within the hierarchy is a further crucial skill. This is paramount for ensuring that efforts are focused on the most critical areas and that resources are allocated effectively. This is where setting clear performance metrics (SLIs), establishing SLOs, and defining escalation procedures across various SRE, engineering, and product teams provides support. By aligning on common objectives and metrics, teams and stakeholders can work together toward shared goals, fostering a culture of accountability and continuous improvement.

The hierarchical structure of reliability engineering is aimed at providing a framework within which reliability goals are defined, implemented, and monitored across infrastructure, application, and business layers. This is done through structuring various teams to collaborate through conversation, using processes developed and implemented within each pillar to foster alignment.

By delineating responsibilities, fostering collaboration, and aligning goals across the hierarchy, organizations can ensure the reliability and availability, within set performance thresholds (SLOs), of their systems and services, ultimately driving business success. In the section that follows, we will briefly discuss concepts within observability and monitoring that enable your technical teams to do just that.

# The relationship between observability and monitoring

Observability and monitoring are indispensable components of reliability engineering, providing insights into system behaviors and performance. The simplified responsibility of this pillar is to ensure that the team can obtain the level of visibility into the system required to gather, aggregate, and synthesize data in a meaningful way. Observability specifically refers to the ability to understand and diagnose system behaviors through the collection and analysis of telemetry data. It encompasses the visibility, traceability, and understandability of system components, allowing engineers to gain insights into performance, latency, error rates, and other critical metrics. Observability is driven by telemetry, or the information and data that is used to produce insights for the respective consumers. You’ll often hear logs mentioned in reference to telemetry data, which is not foreign if you are familiar with handling monitoring responsibilities. Metrics, system performance measurements and traces, and the journey of a request within the system are also forms of telemetry data. Despite not being the focus of the text, it’s an important mention and thus covered a bit more in a later chapter.

Monitoring complements observability by providing capabilities for tracking SLIs and detecting deviations from expected behaviors in real time. It consists of the systematic collection processes used for the aggregation and analysis of observability data to detect anomalies, identify trends, and proactively address issues before they impact system reliability. We internally benefit from investing in both at the beginning stages, resulting in improvements in the quality of information available during later development stages and the incident management life cycle.

Together, observability and monitoring enable organizations to proactively identify and mitigate potential issues before they escalate, thereby enhancing system reliability. Each pillar serves as an indispensable practice for ensuring the reliability of systems and services in a structured manner, enabling your organization to maintain internal alignment and customer centricity. Both are critical to understand in relation to your SLI and SLO process to provide your team with additional insights into techniques and tools for gathering and analyzing observability data, as well as best practices for setting up effective monitoring systems and dashboards for later iteration phases.

## How observability and monitoring contribute to SLIs and SLOs

Observability and monitoring play a pivotal role in the establishment of SLIs and SLOs, which serve as the foundation for measuring and maintaining system reliability. If we do not gather the right metadata within a system in a way that enables productive aggregation and synthesizing, then we will not be able to achieve insights that provide meaningful information regarding the customer experience. Let’s go back to the earlier example of a person having a cold or high fever. We can monitor their temperature with a thermometer, or we can monitor whether they are hotter or cooler with the back of our hand. Both will tell us whether their temperature is worse or better but only one solution will provide us with a precise number that determines whether their temperature is within a specific threshold and requires additional care. This is how we want to think of observability and monitoring as it relates to acting as the foundation pieces for developing and feeding the appropriate metadata into our SLI and SLO metrics and thresholds.

By leveraging observability and its core data practices, organizations can define meaningful SLIs that accurately reflect system performance and health. We’ll then shift toward our monitoring systems, which track these SLIs in real time, providing the data necessary to measure adherence to SLO thresholds and identify areas that require the team’s attention prior to a live incident being logged. Although this is achieved through SLO dashboards, we can take advantage of alerting and monitoring capabilities within an observability or monitoring platform or tool of choice to aid in the following ways:

* Reducing the amount of downtime a service experiences through identifying issues before they occur.
* Improving KPI metrics for the customer experience such as NPS, customer churn rates, and customer acquisition rates through meeting SLOs.
* Improving internal **Mean Time to Detect** (**MTTD**), **Mean Time to Respond** (**MTTR**), and other metrics associated with the handling of live incidents as it relates to effectively detecting and quickly resolving issues.

In essence, the implementation of observability and monitoring provides the necessary insights to adequately create and build the feedback loops needed to iterate and refine SLIs and SLOs over time. This ensures that they remain relevant and aligned with overarching business goals and objectives.

## Techniques and tools for gathering and analyzing observability data

The current state of your organization will determine the techniques and tooling of choice. It is best to always understand the fundamentals, even when procuring a third-party tool to understand your own user requirements, so you can ensure the tool can meet the current and future needs of your organization. Within the current market, at the time of writing this, there exist various techniques, frameworks, tools, and platforms for gathering and analyzing observability data. These range from tools for lightweight log aggregation and metric collection to ones for distributed tracing and performance profiling to commercially available platforms that support the creation and monitoring of SLIs and SLOs or provide a capability of another pillar within reliability engineering.

Commonly used tools for immediate use include Prometheus, Grafana, Jaeger, Elasticsearch, and Fluentd, which enable organizations to collect, store, visualize, and analyze observability data at scale. Additionally, techniques such as structured logging, distributed tracing, and application instrumentation can provide deeper insights into system behavior and performance, facilitating root cause analysis and troubleshooting. Regardless of the tooling, the underlying fundamentals as they relate to how you collect, aggregate, and use data will direct you toward the appropriate product.

## Some best practices for setting up effective monitoring systems and dashboards

Setting up effective monitoring systems and dashboards requires careful planning, design, and implementation. If you are like me and only refer to certain chapters of certain books at the point in time when you need them, then you’ll be glad to see that we’ll cover some best practices within this section. If you are also like me and read through some books in their full entirety, then we will also cover this topic within a later chapter. In a nutshell, organizations should consider the following in order:

1. Start by defining clear objectives and requirements for monitoring, including the selection of relevant metrics for SLIs.
2. Identify key thresholds for each SLI, establishing meaningful targets that align with business goals and external commitments, possibly through an SLA.
3. Deploy monitoring agents, configure alerting rules, and design intuitive dashboards that provide real-time visibility into system performance.
4. Structure communication channels for regular reviews and updates to ensure tooling and processes remain effective and internal teams are responsive to changing requirements and conditions.

In a later chapter, we will cover this topic in a bit more depth. In addition, the previously mentioned steps depend heavily upon the current state of your organization and the implemented tooling of choice. However, both pillars provide the necessary visibility, insights, and feedback mechanisms needed to measure, maintain, and improve system performance over time. This makes both pillars, as well as their respective implementation practices, essential to the practice of reliability engineering and thus your SLIs’ and SLOs’ successful implementation. By leveraging observability data, organizations can define meaningful SLIs and SLOs, set up effective monitoring systems, and establish their own best practices for ensuring the reliability and resiliency of their systems and services.

# The cost of not choosing SLIs

Beyond technical metrics, SLIs and SLOs have profound implications for business outcomes. My previous experience with SLIs and SLOs was specific to the services run by the team and internal product engineering teams. The introduction to both immediately reminded me of KPIs. I still firmly believe today that both are technical KPIs for platforms and services. Their implementations and target audiences just differ.

In a setting where your organization uses frameworks such as OKRs reporting the initiatives set to meet organizational strategies and business goals, you may run into instances where business groups and executives are using KPIs and your technical teams are using SLIs and SLOs to drive their SLAs. Understanding the similarities and differences of each, as well as their usages, will only set your organization up to have improved communication in collaborative environments where technical lingo is no longer a roadblock.

Writing this makes me hope that someone will publish evidence-based materials on their relationships and how executives can utilize each in their organization to more effectively achieve their goals. That’s neither here nor there, just wishful thinking. By aligning SLIs and SLOs with key business objectives, organizations can quantitatively measure the impact of reliability on customer satisfaction, revenue generation, and brand reputation.

For instance, let’s regress back to earlier mentions of **Net Promoter Score (NPS)** for customer satisfaction and customer churn rates. An NPS can inform your management team of the customer experience related to a company, product, or service. This metric might provide additional insight when assessing your overall customer churn rate, or the percentage of customers who discontinued business with your organization during some period using a specific formula. However, how do you then tie this into a specific piece of functionality that was due to poor technical implementation, a code patch, or simply a poor version of a release?

Within daily operations, it’s very easy to focus on the metrics that fit within your silo. However, in my opinion, a more flexible organization will have a system in place to feed each of these metrics into the overall company objective to tell a clear story or sell a specific narrative.

## The impact on business objectives, customer satisfaction, and brand reputation

The impact of inadequate SLI selection extends far beyond technical performance, affecting various aspects of business operations. Poor technical performance impacts the organization through inadequate performing systems. This can result in high incoming requests for customer support and success teams, as well as a high number of internal alerts to SRE and other product engineering teams to address various issues. An extensive period spent experiencing each issue can result in customers abandoning your product, employees leaving your organization, and a host of other problems. Each will also have an impact on financial implications. A product or service performing poorly will eventually result in revenue loss, penalties, and legal liabilities, undermining profitability and competitiveness. Furthermore, downtime and service interruptions can lead to dissatisfied customers, increased churn rates, and negative word-of-mouth, tarnishing the organization’s reputation and hindering future growth opportunities.

In today’s connected and distributed environment, where information is accessible with the click of a button (thereby making the customer experience increasingly critical), maintaining high levels of system reliability is essential for retaining loyal customers and attracting new ones. A single instance of downtime or poor performance can have far-reaching consequences in globally dispersed or startup organizations, driving customers to seek alternative solutions and damaging the organization’s credibility in the marketplace.

Let’s revert to earlier mentions of the NPS. It functions more as a survey mechanism where you receive a score from -100 to 100. Outside of direct responses, it does not truly assess the *why*, only that something was below average, average, or above average. This data can likely undergo synthesizing and aggregation to align with a period of high churn using the churn rate formula. We could then use this period to look at the performance of a specific technical product or service to provide an assessment of the following:

* Whether or not the poor metric was related to a poor-performing service or another mitigating factor.
* Whether the organization experienced poor service performance due to a period of high attrition, engineer burnout, or another human resource issue.
* Whether the degradation in customer success was due to a new feature or piece of functionality that did not meet the standards of the target customer base or merely did not perform as well as other market competitors.
* Whether internal engineering practices and a lack of a decision-making framework, as it relates to critical service availability and performance via error budget monitoring, prevented us from making go and no-go decisions.

Although this is a hypothetical scenario, SLIs and SLOs serve as critical tools for decision-making, enabling organizations to prioritize investments, allocate resources effectively, and drive continuous improvement initiatives that directly contribute to business success. This can either negatively or positively impact your KPIs or organizational goals.

A solid understanding of reliability engineering concepts, coupled with proficiency in SLIs, SLOs, observability, and monitoring, empowers individuals to navigate the complex area of system reliability with confidence and clarity. By recognizing the intrinsic relationship between technical metrics and business outcomes, individuals can drive meaningful change, enhance organizational resilience, and unlock new opportunities for innovation and further growth.

## Accurately measuring system performance and reliability with case studies

To mitigate the risks associated with a lack of or inadequate practices for determining and maintaining system reliability, organizations must consider implementing processes for identifying and selecting appropriate SLIs that accurately measure system performance and reliability. Firstly, it is essential to align SLIs with key business objectives and customer expectations. By prioritizing indicators that directly impact business outcomes, organizations can ensure that monitoring efforts are focused on the most critical areas of concern.

Secondly, organizations should leverage a combination of quantitative and qualitative metrics to provide a comprehensive view of system performance. While quantitative metrics such as uptime, latency, and error rates are essential for measuring technical performance, qualitative metrics such as user satisfaction, response time, and usability provide valuable insights into the overall customer experience.

Finally, organizations should regularly review and refine their SLIs to ensure they remain relevant and meaningful in the evolving technological landscape. By staying agile and adaptable, organizations can respond quickly to changing business needs and market dynamics, ensuring that their monitoring efforts remain aligned with strategic objectives.

## Additional examples and case studies

To illustrate the impact of SLIs and SLOs on system performance and thus your organization’s key objectives, let’s consider a few examples and case studies as selling points.

### Gaming platform and services outage

In 2011 one of the largest brands, **Sony**, experienced a 24-day platform outage due to a security breach through illegal and unauthorized intrusion (<https://www.gamesindustry.biz/playstation-networks-24-days-of-downtime-10-years-ago-this-month>). This resulted in the leaking of approximately 77 million registered users’ personal information and PlayStation credentials being obtained by an unauthorized individual. The PlayStation service being taken offline after not being sure whether financial information was leaked directly resulted in the network outage, with the restoration of certain services a week after the intrusion.

This scenario serves as an example of the need for reliability engineering, which is central to SLIs and SLOs. It also highlighted the need to understand the different system boundaries to identify key metrics to ensure extensive security coverage through implementing security features, as well as the need for a better understanding of points of vulnerability. In this case, the outage stemmed from a malicious attack via a user. However, files were identified on a server within the network, highlighting the need to include security engineers in your workshops. That’s because security engineers can help to implement features such as the following:

* PCI compliance to improve how credit card information is secured and stored to provide additional reassurance to customers.
* Implementing vulnerability scans for additional vulnerability testing.
* Implementing penetration tests to replicate realistic potential attacks and identify security needs.
* Improve clarity surrounding areas such as e-commerce security to detect card skimming and other vulnerabilities to improve security for your customer base.

Through penetration testing and other testing mechanisms, organizations can create environments where they can produce relevant, albeit synthetic, data to report on the performance of their implemented security features.

The outage resulted in a class-action lawsuit that Sony settled for 15 million dollars in the form of free game downloads and additional identity theft protection services for impacted users (Sinclair, 2021). In this instance, the delay in implementing the correct authorization and authentication processes for internal servers and experiencing subsequent data breaches without mitigations highlighted the ongoing challenges, at the time, of ensuring that data is secure and reliable. This also highlights the importance of understanding each component that is implemented or integrated within your system to capture and monitor its performance and reliability.

Security is not often the first area considered when defining SLIs and objectives. However, implementing secure mechanisms and understanding the relevant touchpoints that create the boundaries for securing platforms and services is key to identifying the right metrics to monitor with SLIs. Evaluating security breaches includes various aspects of incident detection, response, and impact assessment. The number of incidents occurring within a defined timeframe can provide your team with an overview of the frequency and prevalence of security breaches, highlighting potential vulnerabilities requiring monitoring.

Severity metrics assess the impact of breaches on confidentiality, integrity, and availability of data and systems, aiding in prioritizing response efforts. **Time to Detection** (**TTD**) measures the efficiency of detection mechanisms by tracking the duration between breach occurrence and detection, with shorter TTD indicating more effective detection capabilities. Similarly, **Time to Respond** (**TTR**) gauges the efficacy of response processes, measuring the time taken to mitigate breaches after detection.

Assessing the scope of affected assets, including compromised systems and sensitive data, offers insight into the breadth of the breach’s impact.

Further understanding the root causes of breaches enables organizations to address underlying vulnerabilities and strengthen security controls. Quantifying the financial costs through key security metrics and SLIs associated with breaches, including direct losses, regulatory fines, and reputational damage, aids in assessing the overall impact and justifying security investments. Analyzing attack vectors and incident response effectiveness provides valuable insights for enhancing security posture and resilience against future threats. Finally, monitoring compliance adherence ensures that security measures align with regulatory requirements and industry standards, mitigating the risk of non-compliance penalties.

These examples not only demonstrate the critical role of reliability engineering and SLIs/SLOs in ensuring stability, security, and long-term viability but also emphasize the importance of focusing on the user persona and user journey of the system to capture customer-centric workflows. We hope the workshop portion of the book helps your team collaborate and have the necessary discussions to further develop indicators and objectives to effectively monitor security-related features and performance.

### Cloud service provider performance issues

In 2021, Google, a cloud service provider, experienced an extremely impactful networking issue due to a failure to monitor key SLIs related to network latency and availability, resulting in performance and reliability degradation (<https://www.reuters.com/technology/google-amazon-several-other-websites-down-2021-11-16/>). This led to customer dissatisfaction and increased churn rates, ultimately impacting the provider’s market share and profitability.

In response to global outages, including disruptions in Snapchat, Spotify, and Google Cloud Networking services, several critical SLIs and SLOs could have been implemented alongside reliability engineering practices to mitigate incidents. In some instances, such as Google Cloud Networking Services, they likely were.

Key metrics for areas such as network uptime, measuring the availability of the network’s internal infrastructure to ensure uninterrupted connectivity on an SLO dashboard, could have helped to identify a potential outage prior to the issue occurring. In addition to tracking response time metrics, measuring the responsiveness of cloud services such as Spotify and Google Cloud to end user requests, and error rate, capturing the occurrence of errors or disruption internal to the infrastructure, could have also helped.

By monitoring these metrics and aggregating their respective data to form queries that report on the user experience (SLIs), companies can work toward achieving uninterrupted connectivity and timely access to their platforms for users. Additionally, defining SLOs for service availability and incident response time sets clear targets for maintaining reliability and levels of operational excellence, as well as quickly addressing any disruptions that may occur, improving the overall customer experience.

To strengthen the reliability of their services, companies can implement various other reliability engineering practices. Some practices we’d like to highlight, although they’re not the central focus of our discussion, include designing redundant architectures with failover mechanisms to ensure continuous service availability in the face of infrastructure failures. If we regress back to monitoring concepts, automating their processes and alerting systems, which play a crucial role in proactively identifying and addressing potential issues before they escalate, also helps to identify weaknesses in the system’s resilience under controlled conditions.

Root cause analysis, which we will cover a bit more in a later chapter, following incidents allows organizations to identify underlying issues and implement corrective actions to prevent recurrence. Finally, capacity planning and scalability efforts ensure that cloud services can effectively handle fluctuations in their workload(s) without compromising performance or reliability. By implementing SLIs and SLOs, along with other reliability engineering pillars, companies can enhance the resilience of their distributed services and minimize the impact of service disruptions on users and clients.

### Social media outage

In October 2021, a popular social media platform, **Facebook**, experienced a seven-hour outage that resulted in the loss of 79 million dollars in revenue (<https://engineering.fb.com/2021/10/05/networking-traffic/outage-details/>). The platform experienced a prolonged outage that could have possibly been mitigated sooner through SLI selection and SLO monitoring. This resulted in widespread user frustration and negative media coverage, damaging the platform’s reputation and credibility in the eyes of both users and advertisers.

In this scenario, the implementation of SLIs and SLOs could have been used to monitor the network performance of components within the infrastructure. When considering network performance, it’s important to measure the percentage of time the network remains operational and able to connect data centers to the internet, with an associated SLO ensuring a high uptime, perhaps targeting 99.9%. Additionally, monitoring DNS server reachability would be crucial, ensuring DNS servers can respond to user queries consistently, aiming for an availability determined by previous instances of throttling. Incident response time could also be tracked by setting an SLO on your agents to determine the detection time when SLO dashboards are implemented and configured to develop anomalies within the network.

If we consider the overall practice of reliability engineering, establishing a **Recovery Time Objective** (**RTO**) is also essential, defining the maximum acceptable duration for restoring network connectivity and service functionality post-outage, perhaps within a time frame that’s ideal for the most critical use cases for your service or platform.

Earlier references were made to capacity planning practices; they are critical to the success of reliability. Proactively managing network capacity to accommodate traffic fluctuations without performance degradation, supported by specific SLOs, will ensure your organization is deploying and utilizing the appropriate amount of hardware and drive resource allocation decisions, ultimately improving profitability metrics over time.

Incorporate SLIs and SLOs that aggregate data on key components of the system such that reports on functionality and usability will enhance the reliability of their network infrastructure for customers.

# Summary

In this chapter, we learned that SLIs and SLOs are essential components of reliability engineering and that they are implemented through observability and monitoring processes, as well as tooling. This helps to provide measurable targets for system reliability and performance as it relates to the customer versus engineering experience, as we explored in this chapter. By understanding the RE hierarchy, its relationship to observability and monitoring, and the long-term and domino effect resulting from not implementing the appropriate indicators, organizations can begin their journey toward system-related customer centricity. We covered this in this chapter as well. Internal engineers can also effectively monitor, measure, and improve the reliability of their systems and services as it relates to incident management and the impact of incidents on the customer, as this chapter explained. We further learned that through careful consideration and implementation of SLIs and SLOs, organizations can ensure they deliver and maintain high-quality, reliable, and resilient products for their customers.

Beginning the journey of defining and implementing SLIs and SLOs as a collaborative organization can seem challenging at first. With a solid understanding of reliability engineering concepts and methodologies, you can approach this with the necessary confidence to encourage your team and broader organization.

By leveraging the capabilities of SLIs, SLOs, observability, and monitoring, individuals can develop robust strategies for defining meaningful redefining system performance, establish relevant SLO thresholds for mitigating incidents before they occur, and implement effective monitoring solutions. Moreover, understanding the business impact of indicators can empower your organization to align technical metrics with organizational goals, driving tangible value and fostering a culture of reliability excellence.

By implementing robust monitoring strategies and regularly reviewing and refining SLIs and SLOs, organizations can mitigate the risks associated with inadequate SLI selection and maintain high levels of system reliability, customer satisfaction, and brand reputation. In the chapter that follows, we will cover the importance of having a team that is responsible for kickstarting this journey and key roles that you may want to include to ensure diverse perspectives.

# Further reading

Here, you can find the referenced articles and books for additional reading about concepts mentioned in this chapter:

* Extensions Built Upon Ancient Foundations. *Perspectives on Psychological* *Science*, 292-314.
* Engineering at Meta. (2021, October 5). *More details about the October 4 Outage*. Retrieved from: <https://engineering.fb.com/2021/10/05/networking-traffic/outage-details/>.
* Google, Inc. (2017). *Site Reliability Engineering: How Google Runs Production Systems*. Sebastopol: O’Reilly Media.
* Reuters. (2021, November 16). *Google Cloud, Snap, Spotify back up after brief outage*. Retrieved from: <https://www.reuters.com/technology/google-amazon-several-other-websites-down-2021-11-16/>.
* Sinclair, B. (2021, April 14). *PlayStation Network’s 24 days of downtime*. Retrieved from: <https://www.gamesindustry.biz/playstation-networks-24-days-of-downtime-10-years-ago-this-month>.
