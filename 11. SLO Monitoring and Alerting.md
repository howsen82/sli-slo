# SLO Monitoring and Alerting

In previous chapters, we discussed the history of reliability engineering and the positive impact it has on a team and organization. We also took the opportunity to work through hypothetical system environments to build SLIs and SLOs for an internal distributed system focusing on different touchpoints within. However, once our metrics are in place, how do we proceed when the customer experience has decreased due to some incident? How do we notify the team of an issue that has occurred or is about to occur with respect to the customer experience thresholds identified? What is the appropriate cadence, timing, and levels of sensitivity to avoid overwhelming on-call engineers or interfering with other consumption-based alerting configured for product engineering teams and their respective on-call rotations?

These questions are answered and resolved through the principles of monitoring and alerting specific to SLOs. You might ask, what are the fundamental principles for configuring SLO alerting? Are they like traditional monitoring practices? In earlier chapters, we mentioned the differences between black box and white box monitoring, as well as building SLOs that represent the customer experience versus alerting on a technical metric that our internal engineers care about.

In this chapter, you can expect to work through best practices for setting up monitoring and alerting for your SLOs. You can also expect to visit examples from previous workshop chapters to work through implementing alerting for SLO thresholds. Best practices surrounding monitoring and alerting configuration already exist. Several concepts within the practice are used as guidelines for standard monitoring. If observability is considered in your organization, then the tooling of choice and language to structure your queries will impact this process as well. Therefore, it is important to understand the goal of this chapter is not to reinvent the wheel but to provide you with sufficient information to leverage the information and configure alerting and monitoring specific to SLOs.

In this chapter, we will cover the following topics:

* Monitoring and alerting for SLOs
* Monitoring and alerting for web application SLOs
* Monitoring and alerting for distributed systems SLOs
* Monitoring and alerting for new features SLOs

Now, let’s review a few topics to ensure that we begin the discussion on the same page. No pun intended.

# Monitoring and alerting for SLOs

In earlier chapters, we defined monitoring as the process you put in place to gather data about relevant systems and check for events that indicate the component being measured is healthy and operating as expected. If monitoring is gathering relevant system data and event checking, alerting is the practice of notifying the relevant on-call engineers of performance degradation, unexpected events, and changes in behavior within the system that have occurred. No quote here, simply a summation.

In [*Chapter 4*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/4), we defined observability as the methods used to aggregate relevant data in a consumable manner to investigate some issue or outage. Observability, which includes logs, metrics, and traces, provides the ability to debug and understand unknown failure modes. Monitoring, on the other hand, focuses on predefined SLIs that track system health in alignment with SLOs. While both are critical, monitoring enables proactive alerting, whereas observability aids post-incident debugging.

In *Site Reliability Engineering: How Google Runs Production Systems*, it was mentioned, “*Without monitoring, you have no way to tell whether the service is even working; absent a thoughtfully designed monitoring infrastructure, you’re flying blind*” (Google, 2016). However, without the appropriate level of visibility into your systems, how do you know you are monitoring the right things, in the right place, in the right way? This questioning led to the belief that the hierarchy of reliability engineering (in [*Chapter 1*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/1) of this book) is best when incorporating observability as an individual pillar and setting the tone for monitoring capabilities. This is depicted in *Figure 11.1*.

![Figure 11.1 – Hierarchy of reliability engineering with observability as the initial building block](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_01.jpg)<br>
**Figure 11.1 – Hierarchy of reliability engineering with observability as the initial building block**

Without the appropriate levels of observability and tooling, which includes querying functionality, coupled with a set of practices as guidelines for gathering and aggregating data, how can we ensure the right level of observability in internal systems to proactively assess system events? How do we ensure internal teams are able to immediately make sense of the data as opposed to collecting random logs and spending hours sifting through debugging and troubleshooting system issues?

As much as I enjoy researching and sifting through logs to debug, it’s never been fun during a live customer-impacting incident, when an immediate response and resolution are expected. This is not the goal of this chapter; however, it is important to mention to correlate certain points from earlier chapters with those within this chapter. It is also to help embed the importance of effective observability, monitoring, and alerting practices to ensure a healthy reliability engineering organization. Now, let’s redirect back to monitoring and alerting.

Within monitoring, we first need to differentiate between two types of monitoring behaviors:

* **White box monitoring**: Monitoring based on metrics exposed by the internals of the system.
* **Black box monitoring**: Testing externally visible behavior of the system from the customer persona.

In *Chapters 7* through *10*, we referenced application and cluster architecture within a distributed environment where the system is multilayered. This means that a single symptom of one behavior is likely the cause of another behavior. For instance, if a node in the cluster is undergoing penetration testing, and a single workload is consuming excessive memory, the on-call engineer would receive an alert of high consumption usage. Asynchronously, there is a pod running on the node experiencing **out-of-memory** (**OOM**) or **OOMKilled** error messages due to insufficient memory resources.

If monitoring and alerting are not in place, the on-call engineer might experience a longer period of debugging the OOM errors and reviewing logs to understand what events are occurring for specific components. They’d then need to determine which resource is overconsuming and stemming from which application workload, if applicable. In an environment where observability practices and white box monitoring are in place, the on-call engineer can immediately review a configured dashboard and see the increase in memory usage occurring before the incident. In another instance, they’d receive an alert that consumption has increased, and the chart would ideally point them directly to the respective workload. In both scenarios, alerting simply on the amount of consumption being used is ideal for notifying staff of an active issue.

When monitoring on SLIs and SLOs, we need to focus on customer-impacting symptoms rather than purely technical resource metrics. In the previous example, alerting on OOM errors reflects a system health issue but does not capture customer impact. Instead, monitoring SLIs such as request latency, error rates, or availability helps correlate high memory usage to actual service degradation, which aligns with SLOs.

Think of it this way: if you or a loved one is sick, we can monitor your high temperature, (a symptom) or we can monitor the behaviors that occur daily that caused the cold and thus high fever (the cause). Are you wearing a jacket outside? Do you wear socks on your feet? Are you eating healthily and taking supplemental vitamins? If we think of the predictability of SLO dashboards with respect to our example, we could monitor behaviors (customer centricity) by monitoring the degradation of eating the typical number of meals and the increase in the number of hours a person is sleeping to predict the onset of sickness. It’s similar thinking.

We can monitor the application workloads and the errors they produce, which remains important to do, versus monitoring the underlying metrics that play a role in providing critical functionality to customers. With regard to SLIs and SLOs, we want to monitor and alert on the features and attributes that help us to understand the *why* behind an application with high resource consumption to help determine whether resource quotas are sufficient, or application architecture and dependencies are at play. We can utilize observability practices to monitor and aggregate data in a way that helps to determine what actions were completed to prompt a negative behavior within the system.

In some cases, we might monitor one, multiple, or all attributes critical to health restoration in a singular manner, which may be sufficient for the staff monitoring your system(s). However, creating SLOs and monitoring on each will help you to naturally aggregate data in a way that focuses on the critical components and impacting behaviors that are important to your customer base and experience. This is done through implementing alerting frameworks that help to detect issues of service degradation prior to an incident occurring or within time periods prior to the customer experience being impacted. This is the overarching goal of monitoring and alerting for SLOs.

While several observability platforms support telemetry collection, not all provide native SLO management. Tools such as Google Cloud’s operations suite, Nobl9, and Datadog offer direct SLO tracking, whereas OpenTelemetry primarily acts as a framework for collecting and exporting telemetry data. Selecting the right tool depends on whether you focus on data aggregation or direct SLO enforcement. The following are some observability tools:

* **Elastic Observability**: A full stack observability solution, which unifies monitoring across cloud applications and platforms, providing visualization capabilities.
* **Datadog**: A real-time observability platform providing infrastructure and application monitoring and log management.
* **Dynatrace**: An end-to-end observability platform that provides unified observability, security, and business analytics capabilities, which include intelligent automation.
* **OpenTelemetry**: An observability framework and toolkit used to structure how you generate, collect, and observe telemetry data.
* **ServiceNow Cloud Observability**: A cloud-based observability platform providing similar capabilities to the prior tooling mentioned. However, ServiceNow consists of a broad product portfolio, making it more appealing to current enterprise customers.

Most of the tooling previously mentioned provides similar capabilities: the technical implementation on the backend and third-party integrations with other platforms and application solutions. As with anything, understanding your own architecture and initiative needs will help you to procure the right products.

There is also an increasing presence of platforms focusing on providing capabilities of pillars within reliability engineering, which includes SLI and SLO creation and management. The goal of this chapter is not to sell to you or raise awareness of any specific product, platform, or tool available. It is to inform you of the importance of its existence and encourage you to navigate the market with fundamentals in mind to procure the right solution for your organization. In [*Chapter 14*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/14), we will cover additional information related to this.

When you find the right solution, the goal is to ensure the appropriate features provided are the features needed to direct your team or organization in the direction of reliability. In the event that there is no solution, I hope this book helps you to begin your SLO journey and build the proper process and system that best suits your organization. Now, let’s circle back to monitoring and cover some concepts specific to SLO management.

## SLO monitoring concepts

When creating SLOs for their respective services and indicators, you want each SLO to maintain a performance level that exceeds the level committed to or expected by the customer. As mentioned in an earlier section of this chapter, the expectation is the threshold where a customer notices the degradation of a service or feature. The objective is an internal mechanism that provides a way to maintain accountability internally and affords the team an opportunity to make some decisions prior to an incident or outage occurring.

SLOs are one effective solution for monitoring your systems or services to maintain your agreed-upon commitments. If we regress to the SLIs and SLOs from *Chapters 7* to *10*, our SLOs consisted of the following components:

* **The SLI itself**: A measurement of the level of performance of the service.
* **The SLO threshold**: The ideal level or goal for service performance we want to achieve.
* **Time cadence**: The SLI length of time for the ideal level of performance to remain at or above its threshold. This is also referred to as the compliance period (Google, 2018).

If we reflect on our SLOs, each fits into one of the following categories: it is categorized as being request-based or windows-based. Request-based takes a *good request versus bad request* approach, while windows-based focuses on measurement intervals that meet a specific level of performance. The components of our final SLO and the category type are information that we need to take into consideration when deciding the best type of alert to configure.

In addition to understanding the structure and categorization type of the SLO created for the creation of our alerts, we also want to focus on another aspect as it pertains to configuration, the relationship types:

* **Event cadence**: The relationship between the alert and the number of SLO events:
  + **Precision**: When the alert triggers, is it an event the on-call staff needs to receive notification for?
  + **Recall or sensitivity**: When the alert triggers, what fraction of events are resulting in an alert? Is this necessary?

* **Event cause**: The relationship between the alert and a specific event that caused it:
  + **Detection time**: What time length exists between the start of the event that triggered the alert and the time the alert was sent to on-call staff?
  + **Reset time**: What is the time length between the period of the event ending and the time of the alert resetting?

We’ll focus on using the mentioned criteria in future examples within this chapter to determine the best SLO alert configurations. It is important to understand that sending alerts too often or for less significant events can result in less sensitivity among the on-call team or other internal staff handling incidents. Like paging within any on-call environment, we want each alert to mean something and require an actionable event. The mention of it here might seem tedious. When considering internal response rates and the handling of active incidents in combination with the health of your on-call staff, dismissal of its importance can have negative long-term results for your reliability engineering organization, and thus internal systems and customer base.

This directs us toward the type of alert we want to use to alert on our SLO. We’ll need to consider the following types of alerts, which are ideal for SLOs (Google, 2018). This does not mean that there are not more types out there, or that your team will not identify additional alerting types and configuration options that work best for your systems.

|  |  |  |  |
| --- | --- | --- | --- |
| **Alert Type** | **Definition** | **Pros** | **Cons** |
| **Target** **Error Rate** | Alerting on a small time window (a few minutes) exceeds the SLO | Good detection time | Decreased precision |
| **Increased** **Alert Window** | Alerting on an increased time window, whether time or budget spent | Improved precision and detection times | Low reset times |
| **Increment** **Alert Duration** | Alerting on the time length the SLO is in violation. | High precision | Low recall and detection times |
| **Burn** **Rate Alert** | Alerting on the speed of the SLO violation occurring | Good precision and detection times | Low recall times; longer reset times than ideal |
| **Multiple** **Burn Rate** | Multiple burn rate alerts configured | Good precision and recall times | Long reset times |
| **Muti Window & Burn** **Rate Alert** | Multiple burn rate alerts with a decreased window | Good precision and recall times | Increased level of complexity and manageability |

Table 11.1 – Alert type chart referencing pros and cons from the Google SRE book

Although there are various pros and cons for each alert type, it is important to understand the initial criteria mentioned, related to your SLOs, to ensure that you get off to a good start with alerting configuration. In addition, this book focuses on SLIs and SLOs, but we’ve also mentioned error budgets in earlier chapters. Your error budget is the opposite of your SLO. A 99.5% SLO will imply a 0.5 error budget. The burn rate indicates the rate at which your service is consuming its error budget in relation to its SLO. A burn rate of 1 means that the service is on track to consume its entire error budget within the compliance period, necessitating corrective actions if the trend continues. It’s essential to monitor this closely to ensure that the service maintains reliability and does not exceed the acceptable error budget.

Now, let’s revisit SLOs from previous chapters and flow through the alerting and monitoring configuration.

# Scenario 1 – Monitoring and alerting for web application SLOs

In [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7), we flowed through a series of steps for building SLIs and SLOs for web applications, as depicted in *Figure 11.2*. For future reference, the persona journey is labeled at the top of the figure, with information about the SLI and SLO to follow.

![Figure 11.2 – Persona journey with SLI and SLO information from Chapter 7](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_02.jpg)<br>
**Figure 11.2 – Persona journey with SLI and SLO information from Chapter 7**

Before structuring each SLO, we were able to define the persona journey and workflow through system components to better map the system flow to the phases within the persona journey. This enabled an improved understanding of which components and metrics played a critical role in measuring system behavior. This process resulted in the following SLOs:

* **Display Product Details**: 99.5% of product pages loaded within 3 seconds.
* **Authentication**: 99.9% of authentication requests succeed with a `200` or `OK` status.
* **Payment Details**: 99.9% of payment attempts complete with a `00` status code.

Within each of the identified SLOs, we immediately notice a common theme. The compliance period is missing. In the previous section, we mentioned the compliance period referencing the length of time the service should meet or exceed its performance threshold. The compliance period is typically measured from one point in time to another. Previously, the term *standard window* was mentioned, and I believe that was more of an opinion. The compliance period should be defined based on the service level expectations and the frequency of performance measurement. For example, it can range from daily, weekly, or monthly, depending on the criticality of the service and customer impact. Rather than using an arbitrary 28-day period, the compliance period should reflect actual data-driven insights, ensuring it aligns with customer expectations and service behavior.

If our data does not provide us with this information, we might simply commit to a 28-day period to utilize it as a base or good starting point. If our internal data signals that customers are experiencing high levels of service degradation, then we may want to adjust the monitoring and alerting thresholds, potentially using shorter compliance periods (e.g., weekly) or more frequent monitoring intervals to capture the degradation more quickly. In other events, the data may inform us that we need to configure a shorter compliance period.

In this instance, we’ll agree to start our alerting for each of the SLOs based on a 28-day period. We want to assess the following criteria for each alert to determine the type of alert we want to start with:

* Does the SLO consist of all three components expected in an SLO? If it does not, do we have sufficient data to assume the missing component?
  + For example, during the missing compliance period, we can utilize the standard 28-day period. We’d answer yes to this question.

* Is the SLI/SLO variation request-based or windows-based?

* Is the relationship between the alert and SLO events or the alert and the event that triggered it more important?
  + This response to this question should guide you toward determining which type of alert you want to focus on, using the *Pros* and *Cons* columns of the alert type chart (*Table 11.1*).
  + If you already have an established monitoring system implemented, much of the data available through it will provide you with the necessary information.

We might then redefine the structure of each SLO to include the compliance period and any other information we feel might be missing. If a workshop process is followed, this will likely arise while finalizing and documenting the SLOs prior to setting up monitoring and alerting. This is not a tedious step, but to serve as a reminder to ensure you capture relevant details of the SLI and SLO prior to deployment, our *Payment Detail* SLI and SLO would then consist of the following information:

![Figure 11.3 – The ideal structure of SLOs to ensure each component is captured](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_03.jpg)<br>
Figure 11.3 – The ideal structure of SLOs to ensure each component is captured

Let’s redirect and flow through other SLOs created in the workshop chapters of this book to review SLO criteria.

# Scenario 2 – Monitoring and alerting for distributed systems SLOs

In [*Chapter 8*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/8), we went through a demo for building SLIs and SLOs for distributed systems, as depicted in *Figure 11.4*. We were also able to identify potential SLIs and SLOs.

![Figure 11.4 – Persona journey with SLI and SLO information from Chapter 8](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_04.jpg)<br>
Figure 11.4 – Persona journey with SLI and SLO information from Chapter 8

Before structuring each SLO, we were able to define the persona journey and workflow through system components to better map the system flow to the phases within the persona journey. This process resulted in the following SLOs:

* **Application Resource Autoscaling**: 99% of `Get RPC` calls are completed within 3s during peak hours:
  + **SLI**: RPC calls completed within 3s during peak hours.
  + **Threshold**: Greater than or equal to 99% error rate.
  + **Compliance period**: Within a 28-day rolling period.

  This generates a client-side request to the website, which interacts with the web microservice API to fetch data from the inventory microservice, and the performance of this interaction is measured by the SLI such as response time or error rate.

* **Node Addition**: 90% of node additions are completed within 15 minutes:
  + **SLI**: Adding a node to a cluster completes installation within 15 minutes.
  + **Threshold**: Greater than or equal to 90% error rate.
  + **Compliance period**: Within a 28-day rolling period.

  Authentication implemented with an auth SDK and sidecar container using gRPC for event-driven communication between services.

* **Dashboard Management**: Dashboard pods maintain an error rate below 1% under normal operating conditions:
  + **SLI**: Dashboard pod maintains a low error rate.
  + **Threshold**: Less than or equal to 1% error rate.
  + **Compliance period**: 28-day window.

  Implemented by using a token system for the payment process via the payment API. We’d also need to identify possible status codes that are returned when a pod is unavailable and not able to accept workloads.

In the previous section, we noticed the compliance period missing from our SLOs, of which the same can be said here. However, our initial SLO for resource autoscaling also includes an additional time parameter. If we refer to *Figure 11.4*, we’ll notice that peak hours are between 10 am and 2 pm. For the purposes of the text, we will not focus on time zones or anything of the sort. However, in a live session and depending on the makeup of your organization and the structure of the team, you’d want to ask the following questions:

* Is this metric specific to a time zone, or is there an expectation to implement an SLO per time zone?

> [!NOTE]
>
> If we refer to the earlier multi-region architecture depicted in [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7), *Figure 7.1*, the infrastructure spanned three regions. This is in addition to the notation of increased traffic being specific to the time zone, to factor this into our query. This will heavily rely on the observability or monitoring tooling capabilities currently in place.

# Scenario 3 – Monitoring and alerting for new features SLOs

In [*Chapter 10*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/10), we went through a demo for building SLIs and SLOs for new features, as depicted in *Figure 11.5*.

![Figure 11.5 – Persona journey with SLI and SLO information from Chapter 10](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_05.jpg)<br>
**Figure 11.5 – Persona journey with SLI and SLO information from Chapter 10**

Before structuring each SLO, we were able to define the persona journey and workflow through system components that were defined to better map the system flow to the phases within the persona journey. This process resulted in the following SLOs:

* **Data Cache Time**: 99.9% of data write transactions are successfully completed in a 90-day window < 4 ms:
  + **SLI**: Data write transactions successfully complete and are verified against metadata
  + **Threshold**: Greater than or equal to 99.9% success rate
  + **Compliance period**: Within a 90-day rolling window
* **Home Page Redirection**: 99.5% of home page redirects in a 28-day period are successfully served in < 3,000 ms:
  + **SLI**: Home page redirects successfully complete in less than 3,000 ms
  + **Threshold**: Greater than or equal to 99.5% success rate
  + **Compliance period**: Within a 28-day window
* **Data Cache Correct**: 99.5% of cache hit ratio exceeds 85% within a 28-day period:
  + **SLI**: Cache hit ratio metric exceeds a performance rate greater than 85%
  + **Threshold**: Greater than or equal to 99.5% success rate
  + **Compliance period**: Within a 28-day rolling window
* **Data Cache Create**: 99.5% of server-side status codes are returned as `304` within a 28-day period:
  + **SLI**: Server-side transactions return a `304` status code
  + **Threshold**: Greater than or equal to 99.5% success rate
  + **Compliance period**: Within a 28-day rolling window

In this instance, each of our SLOs includes a standard 28-day period for the rolling window. However, it’s important to distinguish between SLOs and SLAs. While SLOs are internal targets for performance, SLAs represent external agreements with customers, often including penalties for non-compliance. It is important to include that it is not the only period. There are instances where a 30-day period is sufficient. However, each month within the calendar year consists of 28 days (about 4 weeks), which makes it more realistic when needing to determine between a calendar month and a rolling window. Whichever option your team decides to use, ensure that you have evaluated possible options to better determine what will work best and reduce having to restructure over the long term.

## Establishing SLO alerting criteria

For the purposes of the text, we will focus on creating alerts using information from two SLOs mentioned in earlier chapters and sections of this chapter. There are continuously new products within the software industry being created to improve the adoption of reliability engineering practices and efficiently manage the data consumed and emitted to evaluate the performance of internal systems and processes. Prior to selecting a product that is suitable for you, you’ll need to understand the criteria used to implement SLIs and SLOs, including monitoring and alerting, to compare to internal tooling available to you and products you’re considering for procurement.

In this instance, we’ll use Grafana’s integration with Prometheus as a monitoring and alerting toolkit. While Grafana provides visualization, the actual alerting mechanism relies on Prometheus’ alerting rules and queries. **PromQL** is the language used to query consumable data from Prometheus into Grafana. This chapter does not focus on these aspects nor is the goal to persuade you each is better than a competitor. It’s an important mention to flow through the process.

In this scenario, we’ll shift our focus toward deploying the SLO and configuring an alert for the *Data Cache Time* SLO. In [*Chapter 10*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/10), we mentioned the importance of reading and writing transactions during workflows such as guest checkout. If you need to familiarize yourself with the process and data flow, please revisit the chapter. Our SLO of focus from this chapter is as follows:

* **Data Cache Time**: 99.9% of data write transactions are successfully completed in a 90-day window < 4 ms

For the *Data Cache Time* SLO, there is an expectation for data to be consumed during the purchase process and emitted to the caching system. This improves data efficiency and reduces having to duplicate data entry. Data is updated within the caching system with final changes being sent to the database when needed. The final details are heavily dependent on the configuration and flags used to determine success, but we’ll use the `HTTP 200` status code to confirm a write-back and successful write transaction to the cache system.

The ideal timing for this to happen is within 5 ms, but our SLO is set for 4 ms to ensure we meet a high-performance target. Since the SLO is defined based on a performance threshold (99.9% of requests), setting it at 4 ms ensures that we are consistently exceeding expectations, even under load or anomalies. This also ensures that if a user is using the guest checkout feature, any information captured through user input during a purchase is cached and available for retrieval as soon as possible.

![Figure 11.6 – Account application request and response via cache system](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_06.jpg)<br>
**Figure 11.6 – Account application request and response via cache system**

In the previous section, we outlined the SLO criteria. However, we want to answer questions surrounding the event cadence and cause to help determine the type of alert we want to configure:

* **Event cadence**: The cadence will ensure we are firing the alert for necessary events happening at a frequency that requires on-call staff’s attention:
  + **Precision**: When the alert triggers, the focus is on the speed performance of the caching system. We expect cache miss and have enough data to report an average of 3.5 ms, which is more performant than we need it to be at 5 ms.
  + **Recall or sensitivity**: When the alert triggers, it’s ideal for there to exist several events for some duration of time. As it relates to precision, we expect to experience cache misses that are anomalies, so we want lower sensitivity levels.
* **Event cause**: Focusing on the cause of an event will ensure that when an alert is fired, it is related to causal events. An important aspect of the right cause is also supported by where you measure your SLI within the system:
  + **Detection time**: When the alert triggers, we want it to have been caused by a similar event, with high precision. We are already aware of anomalies; therefore, we want a certain number of events to occur to ensure that when an alert is triggered, it is due to a relevant cause. Detection in this instance is of moderate priority.
  + **Reset time**: When the alert triggers, we have moderate feelings toward how long it takes to reset. We’d want to focus on the average length of time it takes for the events that cause the longest duration of anomalies.

In this instance, we’d want to focus on high precision with lower sensitivity and recall times. If anomalies exist at a frequent cadence, resulting in the team having moderate feelings regarding detection times, we want to focus on an increment alert duration, where the focus is on the length of time the SLO is in violation. Since anomaly errors were already mentioned, which occur for different types of system events, it’s ideal to focus on the length of time the SLI is not performant in comparison to the set thresholds.

### SLO configuration and deployment

When details have been fleshed out formally, the team should use their tooling of choice to deploy the set SLO. In this instance, we are using Grafana’s SLO app integration with Prometheus to track and visualize SLOs. Grafana itself does not manage SLOs directly; it relies on Prometheus or other monitoring systems to gather data for SLO calculations:

* **SLI definition**: Our first step is to configure the SLI information, which provides the ability to input the necessary information from your configured data source. Configured in the following figure is the time window, data source, and SLI query using a time series and reporting the rate of change or deviation from 4 ms. A few things to point out: our SLI was set for a 90-day window, which fell short of the 2,116 hours or 88 days configurable in the system. Although we can easily include the calculations in the query, it is an example of understanding how you want to measure your SLIs and configure your SLOs when procuring a platform.

![Figure 11.7 – The first step of SLO deployment; entering SLI metrics and query](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_07.jpg)<br>
**Figure 11.7 – The first step of SLO deployment; entering SLI metrics and query**

Other fields provided on this page are related to your internal system and architecture. References to the cluster and appropriate container are highlighted. Usually, you will notice options such as the region and namespace provided. In this instance, and since there is a single database handling CDN responsibilities for us, a direct reference to the “frontend” service label is used as a filtering option. Let’s move on to the SLO configuration.

* **Threshold and error budget**: Like the SLO configuration, we want to enter the requirements for our threshold.

![Figure 11.8 – Entering the target and error budget (the error budget is the inverse of the SLO and automatically calculated in Grafana)](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_08.jpg)<br>
**Figure 11.8 – Entering the target and error budget (the error budget is the inverse of the SLO and automatically calculated in Grafana)**

This page will collect the target threshold for your service and automatically calculate the error budget for you. This number is the number of 9s the service needs to achieve to remain compliant. Error budgets are mentioned but not highlighted throughout the text. As a reminder, our error budget is simply the inverse of our SLO (1 – SLO). It is also important to note that the number of 9s you would like to achieve will guide you and your SRE team toward determining the number of on-call staff you will need to support this goal. The increase in the number of 9s will result in more overhead, for budget and staffing. Do not confuse yourself by thinking it is not worth the investment. It is to highlight that with an increase in reliability, there is a correlation to an increase in expenses. This further highlights the importance of understanding internal system architecture, reliability engineering capabilities, and ensuring appropriate resource allocation.

* **Metadata**: Once SLI and SLO information is inputted into your platform of choice, the likelihood is naming conventions or additional metadata is requested. As simple as this might seem, naming conventions are extremely important to the success of SLO management as you iterate over time. In addition, if you plan to offboard your process to internal product engineering teams or other SRE teams, the transition will alleviate some confusion that can arise. In addition, when SREs are on-call, this provides easier abilities to reference the appropriate charts and dashboards during a live incident.

![Figure 11.9 – Name and description fields to attach metadata to your SLI and SLO](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_09.jpg)<br>
**Figure 11.9 – Name and description fields to attach metadata to your SLI and SLO**

In addition, if your team plans to manage SLOs as code, it is likely to utilize infrastructure tooling such as Terraform, Ansible, Puppet, Saltstack and so many other solutions available today to deploy your configurations in your platform of choice. If your solution consists of multiple SRE and other development teams within it, it will help to serve as a reminder related to which objects were created internally to the platform or created by an external tool. This will also help to ease the debugging and troubleshooting process in the event issues arise unrelated to your SLIs, SLOs, and other internal metrics.

Maybe soon, we’ll begin to see more SLO platforms with dedicated plugins or external modules created specifically for them. Grafana maintains a plugin to control its alerting system.

### SLO alerting rules

After submitting metadata for your SLOs, your platform should request information to configure SLO alerting rules. At the time of writing this, most SLO and observability platforms allow you to create fast-burn and slow-burn alerting rules. These rules are used to alert on different rates of error budget consumption over defined time periods, such as rapid (fast-burn) or gradual (slow-burn) consumption. Multi-window and multi-burn rate alerting are also available in the Grafana SLO app. However, there may be a case where alerting is not integrated into your solution, or these are not the standard alerting rules provided. As mentioned throughout various chapters in the book, it is best to understand your SLIs and SLOs, the goals you want to achieve, or problems you want to solve, to better understand which platform or app will provide you with the capabilities your organization or team needs:

* **Fast-burn alert rate**: Fast-burn alerts will configure an alert using the criteria set by the provider. In this instance, the burn rate is at least 14.4 or 6 times the error budgets over x minutes and over the last 6 hours:

![Figure 11.10 – Grafana and most other SLO apps will allow you to generate fast- and slow-burn alerts](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_10.jpg)<br>
**Figure 11.10 – Grafana and most other SLO apps will allow you to generate fast- and slow-burn alerts**

In this chapter, we mentioned *Data Cache Time* as experiencing anomalies, more frequently than we’d expect, in addition to experiencing cache misses, for non-trivial reasons. In addition, we know it takes frequent and various misses to occur when the issue stems from the backend or source database. The engineering team has alerting specific for them, but we want to know when/if this impacts the frontend. A multi-burn-rate alert configured under the fast-burn alert within the app is likely ideal for this.

* **Slow-burn alert rate**: On the other hand, there is also the capability to configure a slow-burn alert, which follows similar rules to fast-burn alerts with a lower expectation of the burn rate and a larger window for the application to experience the burn. If the *Data Cache Time* alert proves to be less important than initially thought, it may be appropriate to adjust the alert’s threshold to a slow-burn rate, allowing for more gradual consumption of the error budget before considering depreciation.

Alert routing is a capability that is not included in the text but is a capability provided by most apps and platforms providing alerting capabilities. This is important to take into consideration if your organization integrates with various apps and tooling during the incident management process. It will help to ensure seamless and unified processes with other teams you need to collaborate with for SLO Management.

### SLO review

A formal SLO review screen will display to provide you with the ability to review and then save your SLOs to a folder of your choice.

![Figure 11.11 – Verifying entered data on the SLO review page](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_11_11.jpg)<br>
**Figure 11.11 – Verifying entered data on the SLO review page**

There is no specific functionality to highlight here, other than the ability to view the SLI, SLO, and alert on a single page. It is also important if your team utilizes an external documentation system to maintain track of information regarding what is configured for later iterations.

It also serves as a last-minute review to ensure everything is in alignment. If nothing more, it is an indication that your SLO journey is heading in the right direction!

# Summary

Monitoring and alerting on your SLOs is as important as effectively implementing observability practices to improve visibility into your systems. To do so, we need to ensure the appropriate mechanisms are put into place to better align with the practices instilled within the specifications and implementation phases. In simpler terms, we need to ensure we have the underlying criteria for our SLIs and SLOs to make more informed decisions regarding the applications and platforms we integrate with for SLO management.

The goal of reliability engineering is to improve the customer experience, but it’s equally important to highlight the impact on your on-call staff and development teams. The monitoring and alerting practices we put in place greatly contribute to reducing on-call and engineer fatigue and improve the efficiency of resolving issues as they arise. When we build better processes to investigate, remediate, and communicate with increased observability, we are better equipped to improve the overall customer experience when we fall short.

Now that we have covered creating SLIs and SLOs and SLO management with alerting and monitoring, we’ll discuss best practices for SLO daily operations in the next chapter.

# Further reading

Here, you can review and read the referenced articles and books for additional reading about concepts mentioned in this chapter:

* Google. (2016). *Site Reliability Engineering: How Google Runs Product Systems*. Sebastopol: O’Reilly Media, Inc.
* Google. (2018). *The Site Reliability Workbook*. Sebastopol: O’Reilly Media.
* PagerDuty. (2024). *Best Practices for Monitoring*. Retrieved from: <https://www.pagerduty.com/resources/learn/best-practices-for-monitoring/>.
