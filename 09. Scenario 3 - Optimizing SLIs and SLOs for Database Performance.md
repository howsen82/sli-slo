# Application architecture

In previous chapters, we referenced the web application deployed in cloud infrastructure architecture throughout the respective scenarios. [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6) elaborated on the process with a bit more detail. Expect this chapter to focus more on the backend of your application and the platform’s performance. If additional detail is needed, please review [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6)of this book for reference.

Our application is hosted across three availability zones of data center infrastructure, which we will refer to as zone-1, zone-2, and zone-3. Each zone is fronted by a load balancer and hosts some microservice in our application layer, as well as the components to run the orchestration layer. In a real-world scenario, we’d consider multi-regions with multiple availability zones, as needed, and have our services run with replicas running in multiple zones. In this scenario, we are not focused on replicas or zones, only that we have single instance microservices that reside in the assigned zone to reference.

Within the multizone cluster, we have a seven-node cluster running Kubernetes orchestration. One node acts as our main node running the workloads, or services, required to ensure the cluster is in a good state. For this chapter, we will focus on the performance of the database services and some of the storage layers. Each node within the cluster is hosting the associated applications and databases:

* Zone-1 node:
  + Account app
  + Account DB

* Zone-2 node:
  + Inventory app
  + Inventory DB

* Zone-3 node:
  + Order app
  + Order DB

The cluster also includes a load balancer within each region that is hosted in its own nodes, which routes traffic to the appropriate backend service. External storage is our storage provider for persistent volumes and volume claims. This architecture is depicted in *Figure 9.1*.

![Figure 9.1 – Multi-region cluster and application services](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_1.jpg)<br>
Figure 9.1 – Multi-region cluster and application services

Each service deployed within the cluster maintains its own database service that is responsible for **create, read, update, and delete** (**CRUD**) data transactions of information for its respective application services. As previously mentioned, for this chapter, we will focus on SLIs and SLOs for the database aspects of the application architecture. This chapter encourages you to think beyond just availability and uptime when measuring database performance, though these remain foundational SLOs. In addition to availability, consider incorporating SLIs such as latency, throughput, and error rates to ensure a comprehensive view of database performance and service reliability.

# Personas and the persona journey

We will begin the development of SLIs and SLOs as we did in previous chapters, by establishing the critical personas and their respective journeys within our system. In the initial scenario, we began the discussion by heavily focusing on understanding the data and how to incorporate it into the discussion while asking the right questions to elicit the right information. In this chapter, we will not focus on that aspect so much, as our architecture is a continuation of the previous chapters, just on the data layer.

If we remember in our initial scenarios, we mentioned four persona types. In this scenario, we are focusing on the “user” persona type and have the following personas, of which we will focus on their journey as it relates to the database application and its performance:

![Figure 9.2 – Personas listed under their respective persona type](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_2.jpg)<br>
Figure 9.2 – Personas listed under their respective persona type

In this scenario, the database is a crucial service involved in numerous workflows. As previously mentioned, it is responsible for retrieving and updating inventory. Therefore, for many users, it’s important that information is accurate, retrieved within an expected amount of time, and updated when needed with the appropriate security implications in place. Each application within our service architecture emits information to its own backend database application, which integrates with a physical storage layer.

The physical storage layer points of presence are within a location associated with the respective region listed in the architecture. Each zone consists of hardware within a data center in the region that is responsible for database services associated with the region. We can also go a step further and include each service and node deployment labeled with the region, so it understands to only schedule and run services with the appropriate labels.

The identified personas that are concerned with the process and are critical to the workflows within our system, referenceable in *Figure 9.2*, are as follows:

* **Application service** – Each service within the deployment is coupled with its own database. For simplicity, we will focus on the shipping app service and database.
* **Backend developer** – For this scenario, the team decided they wanted to get more specific and focus on the backend database, as full stack and frontend developers within the organization are typically focused on the frontend services. They are also responsible for the design and implementation phases.
* **Database admin** – Once the database has been developed, the database admin or engineer is responsible for maintenance and has various concerns surrounding database performance.

Since we have identified the personas critical to our application(s), we want to take a minute to understand the various workflows that help each persona achieve some outcome in our system. The team has also expressed some concerns regarding performance during peak hours and with increased traffic during holidays. Therefore, this is something that we want to take note of when working with the team through the various workflows. It’s also a good idea to incorporate it on a sticky note. Also remember that much of the information will stem from the engineers included in the workshop and any internal data that may be relevant to the discussion, such as incidents, application/service logs, support cases files by the external customer, and so on.

Now, let’s take a step back and shift our focus toward identifying the respective workflows and converting them into the persona or customer journey. For each persona, we will define measurable SLIs that represent key aspects of their workflow. We will then align these SLIs with SLOs, ensuring that expectations are clearly defined for each phase of the journey and that we can measure success in terms of user experience and system performance.

## Defining the persona journey

In *Figure 9.3*, you’ll find each persona listed with the possible journeys. Critical journeys are dotted on their respective sticky note. This indicates that the journey is critical, and we will move it toward the next phase of our SLI and SLO process.

![Figure 9.3 – Critical personas and their respective persona journeys](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_3.jpg)<br>
Figure 9.3 – Critical personas and their respective persona journeys

Of the journeys listed, we find the following persona journeys to be important and would like to flesh out more details to further break down each journey into more detail during the expectations phase. For each journey, we identify the respective persona’s expectations and define the associated SLIs that represent those expectations. These SLIs will then be used to set SLOs that ensure each persona’s expectations are measurable and aligned with the overall service reliability goals:

* **Shipping** **service persona**:
  + **Data retrieval** – The shipping service retrieves customer and product information from the order service to calculate shipping details such as pricing. The database then connects to an additional and external facing service to provide the necessary information to calculate shipping ETAs. The external service then provides the calculated shipping ETA via the API:
    - We could even extend this to an additional consideration of API requests returned based on status codes.
    - Data retrieval can also be included when a customer requests and views the ETA for product shipment. In this case, a freshness SLI should be defined to measure the latency between the most recent read transaction and the corresponding write transaction.

* **Developer persona**:
  + **Database deployment** – Deploying the initial database into the cluster.
  + **Query optimization** – Queries to the inventory database are completed successfully without errors.
  + **Database change management** – Integrated automation processes read compute resources and apply the necessary changes to the database, rolling out a new YAML file. The dev team would like to ensure that the method implemented is completed successfully.

* **Database** **admin persona**:
  + **Database performance backup** – The database successfully completes and is replicated at a specific cadence in preparation for recovery.
  + **Database performance recovery** – The team would like to ensure the external SLAs to the customer are met regarding data restoration.
  + **Database integrity** – Data is consistent through replication of primary and secondary storage systems.
  + **Performance latency** – Ingestion rates perform without degradation:
    - This can also extend to saturation, or the amount of storage utilized in comparison to storage that is available.

Each persona journey in the previous list is an ideal candidate regarding business criticality to the team, according to the internal research and analysis process used to determine customer pain points and temperature. Therefore, we’d want to further analyze and assess each item to identify what the customer expects within each phase of the journey. This step includes the team shifting the selected journeys over to the next section of their whiteboard or the tooling of choice being used to organize the conversation.

![Figure 9.4 – Persona journey identified to understand the expectations within each phase](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_4.jpg)<br>
**Figure 9.4 – Persona journey identified to understand the expectations within each phase**

Now that we have a better understanding of the persona journey and additional context, we can shift the discussion toward expectations. For each persona journey, what does the data inform us of regarding expectations of the system? Are there specific cadences or time lengths? Let’s dive into the next phase of persona expectations.

## Setting journey expectations

In *Figure 9.3*, you will find the persona journey list in preparation for persona expectations. The team is starting to feel a bit more comfortable and decides to add additional items to the board to do the following:

* Add additional discussion points
* Gain a little bit more traction
* Establish a larger pool of journeys to pull from when they iterate through the initial batch of SLIs and SLOs

Our board might include the following information after group discussion, elaborating a bit more on each of the journeys and persona expectations:

![Figure 9.5 – Persona expectations for each persona journey](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_5.jpg)<br>
**Figure 9.5 – Persona expectations for each persona journey**

The team is confident in defining SLOs for the following journeys and expectations, with each SLO to be tracked via SLIs, ensuring measurable performance goals that align with user expectations. For the purposes of the text, we’ll walk through the persona journey phase mapping to prepare for the next step in our process. We’ll start with the expectations for the following persona journeys without the technical and process implications to be established in later steps:

* **Data retrieval**:
  + What is the format and type of data being retrieved from the system?
    - Customers have complaints regarding the reporting of the estimated time of arrival for their product shipments reported in their profile.
  + Are there external dependencies that contributed to the success of the journey?
    - Yes, the dependency is with the third-party system owned by the shipping provider communicating the ETA back to our system.

* **Database** **change management**:
  + What does *successful* mean regarding the deployed workload?
    - The statefulset is deployed into the cluster and reports back to a healthy and running state. All associated resources are ready and able to accept and consume traffic.
  + What is a reasonable amount of time?
    - The support and SRE teams noticed a pattern of customers filing tickets, on average, between 12 and 15 minutes.
  + Are there any other expectations?
    - Internal engineering teams want to encourage engineers to review and submit clean code. Therefore, they want to roll out configuration and other changes without having to resubmit code for the same change.

* **Performance recovery**:
  + What incidents have previously triggered the recovery workflow and for which parts of the system?
  + What SLAs are in place to trigger recovery processes and how do they define acceptable downtime or data loss for each component?
  + How do we know and validate that recovery was successful?
  + If there were failed attempts, how long did it take to notice data integrity issues?

* **Query optimization**:
  + What internal metric indicates that a query is successful?
  + Which query are we most concerned with to start?
  + Are there already internal optimization or reporting mechanisms implemented for specific queries?

To better answer the previous questions and better align the expectations with the respective workflows, let’s start with a persona journey visual for data retrieval. As previously mentioned, this is not a required step. It is a single method that can be utilized quickly during a workshop to ensure that an accurate description of workflows and tasks within each flow is captured.

![Figure 9.6 – Customer journey visual for shipment ETA](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_6.jpg)<br>
**Figure 9.6 – Customer journey visual for shipment ETA**

In *Figure 9.6*, we can immediately see at a high level the phases or steps that are taken for a customer to retrieve data regarding the estimated shipment date. It’s also important to note that the current workflow includes an immediate email notification to the customer regarding successful order completion and potential shipment dates.

However, it does not update the customer regarding what the actual shipment date is once a tracking number has been assigned to the product and a shipment date calculated. The customer is requesting an accurate ETA within 24 hours of product purchase. This has a dependency on the process and integrated APIs with the shipping partner. Let’s shift toward understanding the system boundaries, and how each component works together to achieve the necessary functions for the critical persona.

# Establishing application (system) boundaries

Now that we have identified the critical personas and their respective journeys, it’s time to dissect the database architecture a bit more to understand how each component works together to achieve some outcome. This will help us to ensure that we are measuring the right things in the right way. As previously mentioned, this scenario is going to focus on the database functionality as opposed to the infrastructure layers.

Let’s revisit our persona journey map and sequence diagram to understand the functional and technical steps required to reach the journey’s end. Let’s look at the *Data Retrieval – Shipment ETA* workflow to further establish the components and system boundaries.

![Figure 9.7 – Shipment ETA sequence](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_7.jpg)<br>
**Figure 9.7 – Shipment ETA sequence**

The sequence diagram for this specific persona journey helps to visualize the flow and understand the order of operations between the individual services deployed within the cluster. It’s important to remember that each individual service consists of a service pod and a database pod that encapsulates the container(s). In some instances, both a service and database exist for the same workflow, but we are more concerned with the database and flow of the data to measure performance.

This is mentioned as the pod will host the container, which includes the underlying code from the local or external code repository and any other application dependencies. For a quick reference of the view between the services and cluster components, see *Figure 9.8*.

![Figure 9.8 – The network traffic flow from application to storage](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_8.jpg)<br>
**Figure 9.8 – The network traffic flow from application to storage**

The flow between components includes the application container, which includes the application code and relevant application dependencies. This container or pod is configured with a service object that is configured with networking options to route traffic to specific objects within the cluster based on configuration rules. The service object directs toward the database service, which consists of the database code and configurations for the required database. The database will then direct to a **persistent volume claim** (**PVC**), which consists of the metadata to request storage via the volume object. The volume is an abstract of the physical hardware utilized for storage. The architecture is not the focus of this section but is mentioned to serve as a reminder of the flow of data and ensure that you and the team understand the communication flow.

Another consideration worth noting is the numerous types of storage configurations and architects that can be configured for a cluster. When the kubelet on the node attempts to start the container, the provisioning phase will occur, and the underlying storage will mount to the pod’s container via the volume. This is also important to take into consideration the changes that happen when we complete a task as simple as changing the configuration in the YAML file of an object associated with the database flow.

In this scenario, the object workflow might appear as depicted in *Figure 9.9*.

![Figure 9.9 – Network flow for storage provisioning and attachment](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_9.jpg)<br>
**Figure 9.9 – Network flow for storage provisioning and attachment**

*Figure 9.9* depicts the flow of the storage provisioning through both dynamic and static volume provisioning at a high level. Each flow follows the same pattern, except for in *Step 2*. Dynamic provisioning will flow through the storage class object, which is configured by the administrators. The storage class provides a set of configuration options that are available and mounted when the application and PVC are deployed, and sends the request.

On the right-hand side, you’ll still see the flow shift starting with the reference of the application (pod) to request storage through a PVC. The referenced claim requests a **persistent volume** (**PV**), which it successfully binds to if all objects are available and in a healthy state. The PV in *Step 3* is provisioned through the storage class on the left-hand side, while on the right-hand side, the YAML files are written and deployed requesting the appropriate volume.

The storage plugin interface will connect to the external storage solution, located within the respective data center. Although this is not the focus of this chapter, it is important to mention it to ensure that the overall architecture and workflow are understood. Let’s shift toward the specification step with this in mind.

# SLI specification

As done in the previous chapters, upon understanding the underlying architecture, we want to ensure that the expectations associated with each journey are associated with an SLI type to help guide the conversation and take into consideration the various measurements possible outside of uptime and availability, although both are equally important.

To serve as a reminder, you can view *Figure 7**.8* in [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7) to revisit a brief list of the common SLI types used within this chapter as well.

Based on the previously mentioned architecture and description, we’ll conclude that the team has decided on the following SLI types to work through the specification process:

* **Data retrieval**: Data retrieval in this instance is the estimated shipment date that customers feel should be reported within 24 hours of placing a purchase. The shipping process has an external dependency on the shipping provider, of which their SLAs commit to 48 hours based on the architecture. In this instance, it is important for the organization not to commit to a measurement that does not align with the business completing the task. Based on this information, the team agrees to 72 hours to leave room for error between both organizations, and after analyzing the data and agreeing that 24 hours is simply unreasonable. Therefore, they decided on the following SLI types:

  + **Response time**: The customer is requesting a specific piece of data to return within an allotted amount of time. Regardless of the time we commit to, it’s ideal to have a metric that ensures we honor the time set if it is outside of the parameters requested.

  + **Freshness**: This metric should ensure that the data being retrieved is up to date within a defined timeframe. Freshness is a critical SLI when the system relies on external data sources, such as APIs, to ensure that the most recent data is captured and used in processing.

> [!NOTE]
> 
> Freshness is an SLI type that is not typically mentioned, nor was it covered in the SLI types mentioned in [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7). Therefore, it is important to note here that it focuses on the recency of stored data. This is especially important when retrieving information regarding shipping dates, to ensure the most recent data is captured and being used. In some environments, it can be simply comparing referenceable timestamps of some objects within the system, or in more elaborate environments, it can include other scraping techniques and tooling or a focus on time-based series and measurements.

* **Database change management**: This is with regard to direct changes made to the database via the local code repository and then applied through the respective YAML files. The team also notices the rolling update feature for stateful sets and decides this is a good feature to leverage, which also supports rollbacks:

  + **Errors**: To ensure reliability, we need to track the failure rate of deployments, specifically for database-related changes. This SLI will include tracking the error rates during deployments and code changes, such as failed rollouts, failed database migrations, or failed readiness probes.

  + **Deployment success rate**: The percentage of successful database deployments without rollback.

  + **Error rate**: The percentage of deployment failures (e.g., due to failed pods, and unready replicas). This information is made available through the following:
    - **Ready field**: The number of replicas configured deployed and in a ready state
    - **Up-to-date field**: The number of replicas updated to achieve the configured desired state
    - **Available field**: The number of replicas ready and available to users of the deployed objects

  This is in addition to utilizing the probing capabilities to periodically run checks on containers.

* **Database performance recovery**: The team aims to ensure data consistency and availability during recovery operations due to storage-related bottlenecks. This results in a limitation on the performance of the system network. To meet this goal, the following SLOs should be established:

  + **Replication latency SLO**: Define an acceptable time for data to be replicated across multiple availability zones (e.g., data should be replicated within “x” minutes).
  + **Recovery point objective** (**RPO**): The maximum acceptable data loss in the event of failure (e.g., data loss should not exceed 15 minutes).
  + **Throughput**: Measure the throughput for data transfer rates to and from the storage to ensure that it meets expected performance levels under varying loads, particularly during recovery scenarios.

  It may also serve best to configure the metric based on the complaints via support tickets and data logged by your incident management team to create an SLI and optimize it along the way. The goal is to test and monitor various loads to determine methods for the database to continue to recover after peak usage.

> [!NOTE]
> 
> It’s important to include (and may have already been mentioned in earlier chapters) that some SLIs will make sense based on the persona’s expectations of the journey and the architecture without having sufficient data internally or monitoring and observability configurations to gauge an accurate starting point.
>
> In instances such as this, the team, with agreement from stakeholders and the respective engineering team, can configure a metric and SLO starting point based on incidents and monitor it over a given period for additional optimization at a later point.

* **Query optimization**: Optimization in this instance focuses on the quality, speed, and correctness of the product information requested by the customer or end user on the website. There have been complaints of slowness and missing product-related details, of which the team has already identified metadata discrepancies within the database:

  + **Errors**: At this stage, the team does not have sufficient data to focus on ideal speeds. To start, the team agrees to track the error rate or that of the backend database queries that succeed or fail as a starting point.

> [!NOTE]
> 
> Based on the original requests related to this persona journey, it is also ideal to focus on correctness. However, correctness and query speed/optimization are SLIs that the team can iterate upon once they have spent some time tracking the error rates. This is ideal in this instance where there is not sufficient monitoring or data available to build a solid starting point.
>
> Tracking the success and failure rates of the queries run will help the team identify specific behaviors through errors and which application workflows are associated with specific issues to improve issue resolution. This will also passively help your incident management and support teams in the long run, as it relates to debugging and troubleshooting live incidents.
>
> Based on discussions with the team and the items identified in the previous lists, we might depict our specifications diagram, which includes the SLI types and SLI specifications shown in *Figure 9.10*:

![Figure 9.10 – SLI specification flow from persona journey to SLI specification](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_10.jpg)<br>
**Figure 9.10 – SLI specification flow from persona journey to SLI specification**

In previous chapters, we focused on a single persona journey and created SLIs and SLOs for each. In this chapter, we have identified individual personas and selected a persona journey for each to focus on creating an SLI type. We’d formulate our SLI specifications with the following conditions:

* **Data retrieval (shipping ETA)**: *Figure* *9.7* depicts the sequence diagram, showing the flow of traffic and data while placing an order. An external dependency exists, requesting and sending the shipping ETA via the `/api/getReturnETA,/api/PostReturnETA` API constructing the `returnETA()` method and metric. This might lead us to request the following:
  + Responses from `/api/getReturnETA` are sent within a certain amount of time
  + The `returnETA` metric is updated with information from the API within a certain amount of time or at some cadence via the shipping service’s `/``api/PostReturnETA` API

* **Database change management**: Due to the reporting of application/container status via the container orchestration and the ability to complete readiness checks as a feature, this might lead the team to request the following:
  + Database deployments occur without rollbacks
  + Database deployments are available

  In this instance, we might err on the side of a readiness probe due to the handling of data, but not in the sense of needing to load large amounts of data during the startup phase. If that were the case, we might err on the side of implementing a startup probe.

* **Database performance recovery**: High availability and data recovery are important aspects of this organization. Therefore, replication and data protection are a high priority. The team agrees, and based on customer incident reports, reporting on data integrity and consistency is a key SLO for brand and reputation improvements. They are requesting the following:
  + Replication across availability zones at some cadence for recovery processes
  + Consideration of primary and secondary storage system performance

* **Database optimization**: To start, the team may want to capture success versus failure rates for certain queries, as replication efforts for load balancing will contribute to improving this SLI and SLO.

> [!NOTE]
> 
> We’ll rename this SLO *Database Optimization vs. Query Optimization* to reflect the database activity versus narrowing the activity to optimizing a specific query.

It is imperative to focus on understanding each customer’s persona and journey, so that when you are figuring out why you are measuring a specific component and where, your team does not experience scope creep and stray away from customer centricity. The specification stage of the process aids in ensuring that alignment stays intact. Let us shift toward the next stage and focus more on the implementation phase.

# SLI implementation

Now that we have specified “what” after identifying the “why,” we want to navigate the “how” and “where” a bit more in depth. How do we go about implementing SLI types and determine where it’s best to measure, while also incorporating SLO thresholds? This phase simply allows us to ensure and transition the specification in a way that helps to structure and ensure we are measuring the right things in the right place, in the right way.

It’s important to understand that in a live workshop, the flow is going to shift through conversation and numerous updates. This is not depicted within the text but is mentioned as a reminder for you to incorporate things to add or remove during your own internal workshop, whether in person or virtually.

The figures from the specification toward implementation just serve as a visual aid of the actual flow. You should feel free to incorporate as much or as little information as is needed for your team to synthesize and hypothesize. Now, let’s shift toward understanding how we should incorporate the implementation details.

![Figure 9.11 – SLI and SLO implementation details](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_11.jpg)<br>
**Figure 9.11 – SLI and SLO implementation details**

In the previous section of this chapter, we were able to identify the criteria and technical details surrounding what we want to measure and how. In a live workshop, much of this section is covered through collaboration and conversation among the engineering team and the SRE leadership team. In some instances, final sign-off may be required by stakeholders and other internal leaders to ensure the components measure up to and validate internal KPIs, organizational strategy, business unit OKRs, or any other criteria agreed upon in earlier phases of the process.

Now, let’s imagine our SRE leaders signed off and agreed to the SLOs as well as implementing any architectural changes to support their implementation. In *Figure 9.11*, we identified the following SLOs based on previous data synthesis activities done in earlier sections of this chapter:

* **Data retrieval (shipping ETA)**: 99% of `returnETA` metric timestamps post an ETA within 72 hours of a successful purchase.
* **Database change management**: 99% of **StatefulSets** (**STSs**) are deployed without a rollback.
* **Database performance recovery**: Database instances should process 500 MiB of data per second for 95% of requests within a rolling 28-day window. This ensures performance is consistently measured over a dynamic, continuous time period, rather than a fixed past window:
  + Database replication across clusters is successfully completed within 45 minutes
* **Database optimization**: 99.5% of primary database queries successfully complete within a 28-day period.

For the `returnETA` SLO, we accidentally covered much of the technical detail in the specification step. There is nothing wrong with that. However, I want to revisit and cover implementation for the remaining three:

* Database change management
* Database performance recovery
* Database optimization

If we revert to the technical architecture covered in [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7), in *Figure 7**.1*, it’s helpful to remember the cluster is deployed across regions and *database performance recovery* requires the implementation of additional secondary storage and database instances in separate regions for failover operations. The team understands this will take the engineers some brainstorming and time and agrees to later prioritize it as high business impact, due to several outages, with less feasibility.

For *database optimization*, the team will track query success rates by measuring the ratio of successful database queries to the total number of queries within a defined time window, such as 28 days. This metric will calculate the SLO for optimization as *(successful queries / total queries) \* 100%*. Let’s depict these changes in the prioritization chart of our visual board!

# SLI prioritization

Prioritization in this scenario will follow a similar workflow used in the previous chapters. However, in this chapter, we will need to focus on how the database impacts the respective journey and prioritize based on how the SLI and SLO will impact the broader business and strategic goals. In this instance of prioritization, we have a single SLO for data processing. We must evaluate its impact on system reliability by analyzing past incidents, outages, and the frequency of breaches in the SLO. This will help in understanding how much it affects the customer experience and business outcomes.

To begin, we will want to open discussions with the team to determine how feasible it is for the team to implement. What components within the system are directly involved in the success of the workflow? How do upstream, downstream, and third-party dependencies (e.g., cloud provider services) affect the ability to meet the SLOs? It’s important to assess whether third-party services can meet the agreed SLOs and define their impact on your own system’s SLOs. In the scenarios used within this chapter, we were able to identify and discuss the dependency shipping ETAs have due to being established by the shipping partner.

![Figure 9.12 – SLI prioritization according to business impact and feasibility](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_09_12.jpg)<br>
**Figure 9.12 – SLI prioritization according to business impact and feasibility**

In this instance of prioritization, we have a single SLO for data processing, which requires revisiting the process use and iterating on incidents and outages. On the other hand, two SLOs exist, which will include collaboration and a dependency on a third-party organization and new components added to the system architecture. When prioritizing, the team will want to work closely with engineering to determine the length of time it will take to implement. If looking for short wins, the team may want to identify additional SLIs and SLOs they are able to implement while other tasks are being completed.

# Summary

When building SLIs and SLOs for your application and infrastructure architecture, it’s important to think of the various components at the data and storage layers, and how each can be measured to ensure reliability for your customers. Often, the mind will immediately shift toward measurements surrounding availability and uptime, which are critical to system performance.

However, I hope this chapter has at least highlighted some of the areas and components that need to be taken into consideration to foster thinking outside of the previously mentioned measures of performance.

I also hope that the structure of this chapter helps you to foster an environment where these conversations can be had to efficiently identify areas of importance, and which are central to the success of your customer’s experiences. In the chapter to follow, we will shift the attention toward focusing on new features within your system, as well as iterating through established backlogs. We will focus on making traction using new features identified in this and previous workshop chapters.

# Further reading

Here, you can review and read the referenced articles and books for additional reading about concepts mentioned in this book:

* Authors, T. K. (2024, August 26). *Kubernetes Components*. Retrieved from: <https://kubernetes.io/docs/concepts/overview/components/>.
* Dambrine, F. (2022, December 14). *Kubernetes Icon Set*. Retrieved from: [excalidraw.com](http://excalidraw.com).
* Luzar, D. G. (2023, April 18). *Stick Figures Collaboration*. Retrieved from: [excalidraw.com](http://excalidraw.com).
