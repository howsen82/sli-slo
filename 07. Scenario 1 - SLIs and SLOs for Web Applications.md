# Scenario 1: SLIs and SLOs for Web Applications

In this chapter, we will implement, in more depth, the previous learnings from [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6), with the focus shifted toward building SLIs and SLOs for the web application architecture referenced in the section to follow. When considering the application and build process, we are accustomed to measuring specific metrics to determine what went wrong when an issue occurs. However, it is imperative to understand that SLIs for applications are used to measure the performance and reliability of the system, helping to ensure that the system’s performance aligns with user expectations and impacts customer experience. These metrics reflect critical aspects such as availability, latency, and error rates.

Customer-centric performance measurements also help to direct your engineers toward the components involved in an incident, during the incident’s life cycle. It’s important to clearly understand the goal to communicate the added organizational value.

In this chapter, along with the chapters that follow, we will utilize the same outlined process, working with different components of the previously referenced application and infrastructure architecture. This will help to highlight the SLI and SLO build process for different components of your system.

We’ll be covering the following main topics:

* Understanding the application architecture
* Identifying personas and the persona journey
* Establishing application (system) boundaries
* Specifying SLI types based on the identified system boundaries
* Implementing SLIs with accurate touchpoints based on the system specifications
* Understanding required considerations when prioritizing SLIs and SLOs

Let’s get started!

> [!IMPORTANT] Important note
> 
> In this chapter, we will reference the application architecture depicted in *Figure 7**.1* throughout each step. [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6) elaborated on the SLI and SLO process with a bit more detail. Expect this chapter and those that follow in this part of the book to place emphasis on how the process works with minimal detail. If additional detail is needed, please review [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6).

# Understanding the application architecture

In the previous chapter, we introduced our simple website, which offers cosmetic products to individual customers and businesses looking to purchase in bulk. We’ll continue with this example and scale it to include infrastructure and additional zones, ensuring that our SLOs for availability, latency, and error rates are met across regions. Scaling could be done with a focus on maintaining the system’s reliability, aligning with the defined SLOs. The referenceable architecture is depicted in *Figure 7**.1*.

![Figure 7.1 – Multi-Region cluster with a multi-Region API gateway](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_1.jpg)<br>
**Figure 7.1 – Multi-Region cluster with a multi-Region API gateway**

Let’s discuss the architecture a bit further; to gain a better understanding of the individual components and functionality they provide to the larger system.

Our application and its infrastructure are hosted across three Availability Zones, which we will refer to as **Zone-1**, **Zone-2**, and **Zone-3**. Each zone is fronted by a load balancer and hosts some microservice in our application layer. We use an API gateway for a single point of entry for calls from client applications and backend services for API calls and interfaces. In a real-world scenario, we’d consider multi-Regions with multiple Availability Zones, as needed, and implement the services to run replicas within the necessary Regions and zones. In this scenario, we are not focused on additional replicas or instances, only that we have a single instance microservice to reference that resides in some assigned zone.

Within our multi-zones, we have a seven-node cluster running Kubernetes orchestration. The main node runs the applications and services required to ensure the cluster is in a good state. This includes the following:

* **API-server**: Exposes the container orchestrator of choice and acts as the frontend for the cluster. All client requests route through the API server before routing to other cluster components.
* **Scheduler**: Assigns new pods to a node with available resources and aligned configurations (if labeled).
* **Kubelet**: Node agent, which monitors servers and routes requests to servers.
* **Kubeproxy**: Maintains network connectivity between services and pods.
* **Controller Manager**: Single binary deploying various controllers to a cluster. Controllers call the API server to make updates to an object’s current state to the desired state.
* **Etcd**: Key value store deployment that stores all cluster data.

We also have three additional workload nodes that are running two critical services to the application workloads deployed to the scheduled zone running in a healthy state. The main cluster components are hosted only on the control plane nodes. The following components are deployed to all nodes within a cluster and are reiterated in this section to place emphasis on them:

* **Kubelet**: Node agent that monitors servers and routes requests to servers.
* **Kubeproxy**: Maintains network connectivity between services and pods.

In addition to those services, each node is running the respective containers that contribute to our website’s availability and functionality:

|  |  |  |
| --- | --- | --- |
| **Zone-1 Node** | **Zone-2 Node** | **Zone-3 Node** |
| * Account * Account DB * DNS (system) * Shipping * Shipping DB | * Inventory * Inventory DB * HPA * Payment * PaymentDB | * Order * Order DB * Auth * Auth DB * Website |

Table 7.1 – List of services and the respective zone of the node they are hosted on

The diagramed cluster also includes a load balancer within each Region to route traffic to the appropriate service. External storage is our storage provider for persistent volumes and volume claims, which are attached to the respective application pod. Each application consists of a microservice or pod deployment containing the application code and other app libraries and dependencies. Each application also has its own database deployed, which includes the code and logic regarding the database configuration and necessary data.

As for the individual functionality of each deployed application, each listed application has its own role and responsibilities. This will be discussed in more depth in the *Establishing application (system) boundaries* section of this chapter.

Now that we have a better understanding of the workflows, applications, and infrastructure, we can shift the conversation toward identifying critical personas and defining the persona (user) journey within the system.

# Identifying personas and the persona journey

For this chapter, our focus targets the web applications deployed in the cluster versus the actual infrastructure or platform they are deployed to. Therefore, the personas established for this scenario are going to focus on the web application. If we reflect on [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6), we understand that we want to utilize data repositories likely owned by teams internal to the organization, such as the following:

* Internal incident management teams
* Customer success teams
* Technical support teams
* Product management teams

…and any other internal teams your organization might have that are responsible for managing the customer experience. Utilizing data from customer surveys, incident tickets, and other customer support workflows helps the team identify SLIs that measure the customer experience. By tracking these SLIs, the team can gain insights into performance issues and understand how they impact SLOs, which in turn ensures that the system’s behavior aligns with user expectations. It also aids in understanding which workflows, or functionality, are of importance to your critical customers.

We can then assume that our team is able to generate data and formulate the following personas and their respective journey, from the previous chapter:

![Figure 7.2 – Personas and persona journeys identified in Chapter 6](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_2.jpg)<br>
**Figure 7.2 – Personas and persona journeys identified in Chapter 6**

Before moving further, it’s important to highlight sticky notes are color-coded to better organize and visualize focus areas as we move through each step of the process:

* Green note: Represents the persona
* Pink note: Represents the journey
* Purple note: Represents an expectation
* Yellow note: Represents a dependency
* Orange note: Represents a hypothetical question raised by a participant

For the purposes of this chapter, we will focus on the following personas and their respective journey on our updated board:

* Product purchase journey
* Product shipped journey
* CRM issue creation and assignment journey
* Website availability journey

Each column heading is an individual person’s journey, which is a good starting point for the team to begin their SLI and SLO process. See *Figure 7**.3* for an example of how this might appear on a virtual or in-person whiteboard. In a workshop setting, it is beneficial to the team to use the best method to indicate which items from each stage are candidates for the next step. In this scenario, a single white dot is used to represent the persona journey we will use to begin identifying associated expectations.

![Figure 7.3 – Persona expectations we will use to shift through the SLI process](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_3.jpg)<br>
**Figure 7.3 – Persona expectations we will use to shift through the SLI process**

In a collaborative environment centered around communication, it’s important to emphasize and notate dependencies and parking lot questions in a way that is consumable to the team, especially if done in a virtual setting. In *Figure 7**.4*, you will see an example of a few outcomes from a discussion surrounding the expectations a consumer has for each of the associated persona journeys.

When having a discussion, it is important to document any dependencies that one item might have on another. For example, in *Figure 7**.4*, the success of fulfilling a customer order, including shipping and delivery, involves dependencies that might fall outside the scope of the application, such as external third-party services (e.g., shipping companies). These dependencies should be captured as external SLIs or accounted for in the SLOs to ensure that service targets remain realistic and reflect the entire customer experience. Often, when approaching these instances in discussion, it is beneficial to utilize a process that uses a step-by-step approach to list and visualize them to provide additional clarity and understanding when iterating in later stages.

![Figure 7.4 – An example of dependencies, expectations, and questions for each journey](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_4.jpg)<br>
**Figure 7.4 – An example of dependencies, expectations, and questions for each journey**

Before we dive into each of the personas and their respective expectations, it’s important to remember that you can use other tooling and processes to visualize or document your persona journey. This is out of the scope for the book and the selected tooling is utilized out of convenience and choice.

However, if we use CRM integration as an example, based on the persona journey depicted in *Figure 7**.4*, the customer journey visual would consist of the information depicted in *Figure 7**.5*, utilizing a phased approach to working through the steps within the persona journey.

![Figure 7.5 – CRM issue creation and assignment persona journey](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_5.jpg)<br>
**Figure 7.5 – CRM issue creation and assignment persona journey**

We’d then use the preceding information to either diagram the technical process between the systems or flesh out the details through group discussion to begin documenting the process workflows in preparation for SLI specification. It is also beneficial to use the visual depicted to help the team map out the functional processes and improve the ability to map the underlying application components that work together to create the journey. The charted visualization is not necessary but helpful to ensure all phases are captured in a consumable manner for the teams involved.

In this instance, we are going to shift our focus toward the product purchase persona. If we visualized the phases within our journey chart and diagrammed them, the phases required to achieve an outcome would appear as follows for the team:

![Figure 7.6 – Product purchase persona journey](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_6.jpg)<br>
**Figure 7.6 – Product purchase persona journey**

An important requirement during this phase is to gather the metrics currently configured within your system. It is even more helpful to understand what each metric means to the customer relationship and the components that emit the associated data. For our example, here, we have identified the following metrics that will help to guide the SLI specification and implementation processes:

* **Confirmation_time**: This should be tracked as an SLI to measure the latency from the successful payment submission to the display of the purchase confirmation. The target for this SLI should be defined in the SLO, indicating the acceptable time range for customers to receive confirmation (e.g., 95% of requests should be completed within 2 seconds).

* **Email_confirmation**: The length of time it takes for the email to send the purchase confirmation to the email address associated with the customer account. (It’s important to note the time calculated is how long it took for the system to send the email as opposed to how long it took for the customer to receive it, as calculating the latter can have additional blockers and dependencies outside of what the business can control.)

* **Shipping_eta_response**: The length of time it takes for the customer to receive an ETA for their product after a successful purchase. (It’s important to understand that in certain architectures and depending on the business’s shipping logistics, there can be dependencies that are outside of the business’s control.)

A better understanding of what metrics currently exist, and their respective configurations, improves the ability to gauge user expectations regarding when an issue is noticeable and when complaints or tickets become of concern. The data, through event logging, can also provide better visibility into how components work together, which leads us to the next section.

Bookmark

# Establishing application (system) boundaries

Based on the previous steps, depicted in *Figure 7**.6* of the previous section, we outlined for the product purchase journey, we want to understand and possibly visualize what this means from a technical perspective. If we refer to our application and system architecture from earlier in the chapter, then we already know there are a few application services that are involved with the product purchase workflow. We can assume the following services contribute to a successful customer experience:

* Website microservice
* Inventory microservice
* Account microservice
* Order microservice
* Payment microservice

It’s important to note that microservice, in the context of each scenario, refers to the containers that support the application and the database components together.

At first glance, a team member can assume that we want to measure a single metric for a single service. The concept of SLIs should be tied to specific, measurable aspects of service reliability at each touchpoint, not just the workflow. Instead of focusing on all components collectively, it’s important to measure the reliability of each individual service, especially when it influences the user experience. For example, the latency of the website microservice or the error rate in the payment microservice should be the focus of individual SLIs. The workflow can then be modeled as a sequence of SLIs that align with SLOs across each service.

It’s possible to have a discussion during the workshop with the technical teams that are experts on application architecture. The technical teams may also have design documentation accessible for the group to review together. Based on our architecture, the flow of touchpoints appears as follows:

![Figure 7.7 – Product purchase sequence diagram for purchase workflow](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_7.jpg)<br>
Figure 7.7 – Product purchase sequence diagram for purchase workflow

Based on the sequence chart depicted in *Figure 7**.7*, we can conclude the interaction or workflow is triggered by the customer and includes interactions between five microservices hosted on our platform. Each microservice depicted in the figure includes the application service and the database service as well. Therefore, a good starting point for measuring the length of time would be the client interaction, continuing through each phase of communication between each service. The touchpoint that ends the workflow would involve the payment microservice’s response to the customer, which concludes with an order and payment confirmation communication being provided.

We can take this information and move on to the next step within our SLI and SLO build process.

# Specifying SLI types based on the identified system boundaries

Let’s reflect on the discussion in [*Chapter 6*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/6) about how building the right SLIs and SLOs requires the team to determine the appropriate specifications for what they are measuring. In that, we also mentioned the four golden signals of monitoring (Google, Inc., 2017) being a good starting point if there is not enough data available to develop an SLI or if you are unable to determine a good SLI type. In addition, if your organization or team already has a monitoring system in place, as a reminder, it is good to work backward.

Let’s first revisit the various SLI types, depicted in *Figure 7**.8*.

![Figure 7.8 – SLI types that are good starting points when building your SLIs](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_8.jpg)<br>
Figure 7.8 – SLI types that are good starting points when building your SLIs

The types are not limited to what is listed here and you should also feel free to think of or create your own SLI types as needed by your team or organization. Some other types that are not depicted here but can be taken into consideration are the following:

* **Quality**: The percentage of requests that are processed and correct without the service experiencing degradation.
* **Coverage**: This type captures the percentage or amount of data processed that is also valid.
* **Correctness**: This type captures the rate at which retrieved data is populated in the correct format, type, and various other configuration options available.
* **Freshness**: This type captures the recency rate of data. In many environments, this may be measurable through timestamps.

Some types will also perform better when coupled with the correct functionality. For example, request and response measurements will likely pair better with availability and latency SLI types. Another example is, when measuring storage, durability and throughput might stick out more than others. That’s not to say this is a one-size-fits-all scenario, but it is important to understand what your data is informing you of and what your organization needs you to measure.

Based on our sequence diagram and our SLI-type chart, let’s say the team identifies three SLIs they want to focus on. When starting out, the recommendation is to use a three-by-three mindset. Select three user journeys to start and then attempt to build three SLIs per journey. When working, it is common for ideas and communication to flow; we can utilize the parking lot method to revisit the item later.

For this scenario, we are going to continue to focus on the product purchase user journey and refer to the sequence diagram to identify the areas that we want to focus on. The team decides to break the product purchase user journey into the following three focus areas for SLIs, based on internal incidents, customer support tickets, and application and server-side logs:

* **Display product details**: A client-side request to the website, which interacts with the web microservice API to retrieve information from the inventory microservice, which includes a backend database.
* **Authentication**: Authentication implemented with an auth SDK and sidecar container using gRPC for event-driven communication between services.
* **Payment details**: Implemented by using a token system for the payment process via the payment API.

We might then decide that, based on our data, we want to focus on response time, errors, and latency for the selected SLIs, respectively. In a workshop setting, it’s important to understand these decisions are made through communication in a collaborative setting. The one thing that remains consistent is we want our decisions to reflect the narrative of what our customers are experiencing, as well as what the data from the requirement-gathering process has informed us of. Moments will arise where data is insufficient or even nonexistent – we can refer to other alternatives, such as generating synthetic data. Some examples of generating data in our current system environment are as follows:

* Generating authentication credentials to test security mechanisms
* Generating product purchase workflows to test spikes in workloads

*Figure 7**.9* depicts the selected journeys, their SLI type, and a brief description of the associated workflow, which includes the targeted expectation based on customer feedback and data emitted in various applications and tooling such as application, client, and platform logs.

![](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_9.jpg)<br>
Figure 7.9 – SLI specification details

Based on the data analysis process, we might conclude the following for each specification:

* Customers, on average, submitted tickets to the support team after numerous attempts at reloading a slow-loading product page. However, after the debugging process with the support and customer success teams, customers noticed issues with loading, when the page took seven seconds to load.
  + **Response time**: The team would like to remain conservative and decides it is best to ensure the page loads within five seconds or less.
* The organization has also had complaints to the internal technical account managers from their top enterprise accounts regarding failed authentication attempts. In addition, the IAM security team experienced an outage related to authentication.
  + **Errors**: The successful authentication rate becomes a critical priority, with the expectation of maintaining a 99.9% success rate, with an error budget that allows for a 0.1% failure rate.
* The internal engineering team responsible for the payment service and data reports an increase in service-related incidents. They also include complaints being escalated from the support team regarding the processing of payment systems. As a result, there have been numerous customers abandoning product purchases after several attempts with payment processing failing or experiencing a delay. Customers report it is hard to purchase because, in some instances, there have been duplicated charges reported via their payment method.
  + **Latency**: As a result, it becomes increasingly critical to define a clear SLO based on specific thresholds for successful performance. In this instance, we would not want to focus solely on the length of time for payment processing but be able to provide a concrete SLO that ties directly to an SLI. For example, the payment service should successfully process 99% of payment requests within 2 seconds.

Now that we have successfully identified the problems we want to address, the measurement types, and the internal data and metrics that we can use to support our reasoning, it’s important to seamlessly move from specification to implementation and focus more on the architecture and technical details of the system.

# Implementing SLIs with accurate touchpoints based on the system specifications

The SLI implementation stages shift the functional details of the specification process to the architecture and technical details. The goal of this step is to redirect from the “what” toward the “how” and “where.” This also helps to ensure what is measured, via metrics, reports on the customer experience or a specific workflow versus reporting on a single specific detail that reports technical information for the backend team. That’s what monitoring is for.

*Figure 7**.10* depicts the shift from specification, related to SLI types, toward implementation related to the technical details. Based on the data referenced in the previous section, we know we want the following:

* The product pages load within 5 seconds
* Successful authentication requests
* Payment attempts are successfully processed within 2 seconds

![Figure 7.10 – SLI types that are good starting points when building your SLIs](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_10.jpg)<br>
**Figure 7.10 – SLI types that are good starting points when building your SLIs**

If we refer to the architecture and service deployment in *Figure 7**.1*, then we understand the following services are deployed related to each SLI specification that we want to implement:

* **The product pages load within** **5 seconds**:
  + Website UI
  + Inventory service
  + Inventory database
  + Client-side request

* **Authentication success rate**: The percentage of authentication requests that successfully authenticate users within an acceptable time window (e.g., 99.9% success rate):
  + Client-side request
  + Authentication service
  + Authentication database

* **Payment processing latency**: The time it takes to process payment details from the client-side request to the completion of payment confirmation. A reasonable SLO for this might be 95% of payments processed within 3 seconds:
  + Client-side request
  + Order database
  + Order service
  + Payment service
  + Payment database

We now want to think about each of these services as providing an outcome for the respective persona and their journey. We can also refer to this as a system capability.

Bookmark

# Understanding required considerations when prioritizing SLIs and SLOs

SLIs and their respective SLOs should be prioritized based on business impact and feasibility. Business impact refers to how directly an SLI contributes to customer experience, revenue, and customer satisfaction. Feasibility includes technical complexity, cost, and resource availability, as well as how easily the team can monitor and respond to the metric. For instance, prioritizing authentication success rate over payment processing latency may have a greater business impact if authentication issues are causing users to abandon the workflow. When considering the business impact, we want to ask ourselves the following questions:

* What is the level of impact this change brings to the following?
  + Our customer bases
  + Our team
  + Our organization

* Does the impact affect the business from a monetary standpoint?

  + If so, how?
    - Is this a SaaS offering?
    - Is this a licensed offering?

  + Have we assessed industry competition?
    - If so, does our solution offer something that everyone else’s does not?

* Regarding feasibility, consider the following:
  + On a scale of 1 to 5, how easy is the technical implementation?
  + What does feasibility mean to the technical team members?
  + Are there other solutions available to achieve the desired outcome?
    - This also includes weighing the number of engineers and other staff the implementation might require

The ranking system is based on internal dialogue between the individuals leading the initiative and the technical staff responsible for the respective technical components or designs. In our instance, we might consider the following:

![Figure 7.11 – SLI prioritization based on business impact and feasibility](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_07_11.jpg)<br>
**Figure 7.11 – SLI prioritization based on business impact and feasibility**

There is no prioritization focused on which trait is of importance. That should be determined by the team based on the business and technical requirements of each SLI and SLO. In this example, we focused on building out three SLI and SLO metrics. However, it is also possible to work through this same flow, add items to the prioritization chart, and then loop through the process again, increasing the number of items the team will manage before implementation.

# Summary

In this chapter, we reviewed our example application and infrastructure architecture, worked through identifying personas, created journeys, and gained an understanding of the application boundaries to build SLIs and SLOs in a workshop setting. In the chapters that follow, you can expect to do the same while shifting the focus to different layers such as cluster and data components.

As you continue through the book, I hope you can think of several implementation ideas and other use cases internal to your organization.

# Further reading

Here, you can review and read the referenced articles and books for additional reading about concepts mentioned in this book:

* Dambrine, F. (2022, December 14). *Kubernetes Icon Set*. Retrieved from Excalidraw Libraries.
* DoxyGen. (2024, August 22). *Status codes and their use in gRPC* . Retrieved from: <https://grpc.github.io/grpc/core/md_doc_statuscodes.html>.
* Google, Inc. (2017). *Monitoring Distributed Systems*. In B. Beyer, & R. Ewaschuk, SRE-Book. O’Reilly Media.
* Luzar, D., & Gian, P. (2023, April 18). *Stick Figures Collaboration*. Retrieved from Excalidraw Libraries.
* The Linux Foundation. (2024). *Kubernetes Components*. Retrieved from: <https://kubernetes.io/docs/concepts/overview/components>.
