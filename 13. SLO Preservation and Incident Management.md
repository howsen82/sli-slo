# SLO Preservation and Incident Management

In previous chapters, the attention was surrounding reliability engineering, SLI and SLO management, and various other topics. At the time of writing this book, the adoption of digital transformation is still a priority among many organizations. This is in addition to much attention being directed to the advancements of artificial intelligence and machine learning concepts to further advance the capabilities of software and technology.

Although none of the previous topics mentioned are centered around this book, I have a special interest in learning about and understanding new software and technology to better understand how it can improve processes in my personal and professional lifestyle. Initially, I wanted this chapter to center around how my previous team leveraged the incident management process across our product engineering domains within our organization to validate live incidents against what our SLO dashboards were telling us.

The goal of this chapter is to elevate the role of incident management within reliability engineering and better understand how to improve and leverage it to maintain improved internal housekeeping for your SLOs. We will discuss the relationship between on-call engineers and agents in incident response and resolution within SLO frameworks.

We will cover the following topics as they relate to leveraging incident management as a part of your SLO preservation process:

* Incident management and core concepts within it
* The importance of incident response within the process
* Various techniques for SLO preservation
* The importance of extending accountability to on-call agents and product engineering teams

Let’s take a moment and highlight incident management as a practice as well as the role it plays within an organization as a standalone practice. In the sections that follow, we will highlight approaches for SLO preservation through your incident management practice.

# Incident management in a nutshell

Your incident management system and its impact on the broader organization and your target customer base is only as good as your incident management plan and process. Understanding the significance of SLOs within incident management and incorporating on-call engineers and agents into the SLO framework helps to ensure that service reliability is maintained and measured against defined SLIs.

It passively improves the customer experience and presents to your target customer base a more uniform and aligned organization when issues arise. If we then extend additional accountability to product engineering teams, organizations can enhance their ability to respond to and resolve incidents effectively. This is done in a way that fosters the improvement of reliability and availability through development insights.

Prior to an organization considering SLOs for incident management activities, it is ideal to take a step approach to identify and ensure the following information exists:

* The definition of an incident and its life cycle according to your organization
* Incident management roles and responsibilities
* An incident management communication plan
* Incident response guidelines and a value system

Despite the size of your incident management team, it’s important to ensure that all individuals are on the same page. This chapter does not take on the workshop approach seen in earlier chapters. However, you can utilize a similar structure and flow of the workshop chapters to cycle through creating an incident management plan and SLOs for specific incident management metrics.

## Incident management as a practice

The practice of incident management can be implemented within an organization as a process that is used to resolve issues that arise. It can function as a practice with more elaborate processes or span across an organization where various teams and business groups are leveraging a similar process and tooling to resolve incidents seamlessly. There is no one-size-fits-all approach, only implementing what is needed for your short and long-term goals in a scalable manner.

Incident management focuses on the management of incidents in a way that helps to address and prevent future incidents through process improvement. Incident response focuses on handling and mitigation as it relates to the incident when it arises and its customer impact. This will be discussed a bit more in the section to follow. However, as a practice, incident management consists of six phases, as follows:

1. **Incident identification**: This step includes the detection and report of an incident, consisting of the metadata and identifiers to associate with the incident.
2. **Incident categorization**: This step involves classifying incidents based on predefined categories such as severity levels, which directly impact the SLOs and SLIs. Categorization should align with how the incident might affect SLA performance.
3. **Incident prioritization**: This step includes a metric system that categorizes incidents according to other items and their immediate or long-term impact on the customer and thus business.
4. **Incident response**: This is the process that is used to address mitigation and resolution of an incident. This can vary drastically within different organizations and industries.
5. **Incident closure**: This is the process or steps that follow the incident resolution. This usually sets the tone for the steps to follow to prevent the incident from occurring again.
6. **Learning and improvements**: This includes the process for improving the system from a technical or process perspective, as well as the actions taken to learn from an incident. This is typically done through post-mortems or retrospectives.

The fundamental outcome of your incident management system should be to quickly and efficiently mitigate and resolve issues in a way that fosters continuous learning and improvements. Within each step or phase, the tasks and processes followed will vary and consist of things that make the most sense to your team or organization. Fundamentally, many of the steps within incident management have industry standards or best practices available to follow or utilize as guidelines to get you and your team started. An important step within the management process is how you respond to and mitigate an active customer-impacting incident, which we will discuss a bit more in the next section.

## Incident response within incident management

Incident response is a step within the incident management process. It’s one of the most critical steps, as it relates to how you mitigate an issue from detection through resolution. It consists of issues that are actively impacting the customer and require immediate attention. Like incident response within other industries such as law enforcement, healthcare, and so on, the incident itself is important due to the impact it has on the target audience. How it is handled by the on-call staff is critical to maintaining the customer’s or some individual’s safety. Each area will thus have its own set of guidelines and rules that need to be followed to reach the end goal.

The overarching goal of incident response is to ensure the service stays within its SLOs by quickly mitigating incidents that could cause a breach in the SLA. Incident resolution should focus on restoring the system to an acceptable level of performance defined by the SLOs. With that said, the incident response phase consists of five steps, which are referenced here:

1. **Incident detection and reporting**: The stage at which your detection process or system user detects and reports the deprecation of an activity.
2. **Incident assessment**: The initial assessment of the reported incident. Additional data gathering and the establishment of communication typically happen here.
3. **Incident mitigation**: The tasks completed to mitigate and reduce the impact of the incidents. In more critical events, this can include workarounds.
4. **Incident recovery**: This step includes the restoration of specific components within the system to be monitored to ensure it is operating as expected.
5. **Post-incident analysis**: This typically includes the incorporation of a retrospective or post-mortem event. This includes gathering those involved in the incident to brainstorm through associated events.

Institutes such as the **National Standards and Technology Institute** (**NIST**) and the SANS Institute are specific to security and offer a set of guidelines for incident management. However, within your own team or organization, or even through the development of your SLO process, you might find that there are areas that require additional depth, or the removal of specific concepts is required to fit into your current narrative.

It’s important to understand that best practices and guidelines are just that. If your process helps you identify something different or identifies gaps that require attention, reinvent the wheel, and contribute to elevating the role of incident management, no pun intended. So, what does all of this have to do with SLO preservation? Let’s continue to find out!

# Elevating the role of incident management in reliability engineering

The goal of this chapter is not to persuade you about the best practices for incident management or how to structure your organization for incident response. It is, however, to set the tone for what it is and its relationship within reliability engineering to SLOs. I prefer to start with a brief overview to ensure that we understand core concepts and terminology. With this information at hand, it should help your team to advocate for a culture where incident management is seen as a benefit and complement to ensuring a customer-centric culture with regard to service reliability.

Having experience with support and incident management within various-sized organizations, preservation through SLO is not only critical to a customer-centric environment but also to preserving your staff and reducing engineer burnout. It is equally as important as other areas due to the following reasons:

* Engineers and other staff participating in your on-call rotations are your employees too!
* On-call staff must switch between being reactive and proactive. This is difficult at times.
* They often must manage the customer temperature with minimal information during active events. This can be stressful.
* If your organization does not consist of an elaborate incident organization, chances are they’ve experienced engineer burnout more than a few times.
* They have a more realistic perspective of your product, service, or system, whether you like the opinion or not.

These reasons are significant enough for organizations to begin to shift their perspective on the way incidents and thus support requests are handled. For the organization to truly be considered customer-centric, we need to consider all customer-facing channels. Thus, SLO preservation practices will also be of benefit to your internal engineers and other staff.

## Significance of SLOs in incident management accountability

SLOs play a pivotal role in incident management accountability. When the organization maintains clear and measurable SLOs for incident management such as response and resolution times, while linking SLO performance to incident management metrics, it will experience a correlated increase in accountability of various teams and improved reliability of internal processes outside of just your systems and services.

Clear SLOs encourage implementing teams to not only establish expectations for the services they build but also for the processes used related to incident handling, providing teams with an improved ability to prioritize tasks, allocate resources effectively, and maintain service reliability and availability. SLOs provide a tangible framework for assessing the performance of incident response teams and holding them accountable for meeting defined targets. Areas within incident management that teams can utilize SLOs for are as follows:

* Incident response times
* Incident resolution times
* Incident detection times
* Incident acknowledgment times

There are various other metrics available related to incident management and outside-of-system performance, which your team can utilize to improve incident handling internal to your organization.

The SLOs in this capacity should stem from the targets expected by key accounts, business requirements, or an established SLA provided to accounts from the organization. By setting clear objectives for on-call performance, organizations can ensure consistent service levels by creating and upholding reliability commitments through processes implemented for incident-related activities.

On-call agents are essential for ensuring timely incident response and resolution. They play a critical role in monitoring system health, detecting incidents, and coordinating efforts to resolve issues promptly. SLOs and SLO dashboarding can better aid your on-call agents in the following ways:

* Identifying issues before they are customer-impacting.
* Delegating and reprioritizing technical work as needed and based on current SLO threshold limits.
* Identifying trends over time.
* Ensuring the customer experience is in alignment with agreed-upon service performance metrics.

This increases the need for frameworks or process implementation that meets your on-call engineers’ needs and contributes to avoiding agent burnout and possibly increased employee attrition resulting in increased turnover rates.

In earlier workshop chapters, we mentioned excessive customer complaints and incoming requests related to platform outages and performance degradation in specific critical customer workflows. In addition, the application maintains a global presence, which results in having to consider peak hours during holidays that span several countries and time zones. This results in a requirement for additional attention to international data centers outside of the local region, with different policies and procedures.

The current incident management teams use a *follow the sun* on-call rotation. This means instead of engineers following a 24x7 on-call rotation, on-call responsibilities are rotated between global teams during normal business hours with a separate rotation for on-site infrastructure issues. For purposes of the text, *follow the Sun on-call rotation* is when agents are distributed in various regions where the standard business hour working period is structured to maintain 24x7 coverage. Do not confuse regions with being local to the respective data center.

To increase visibility and improve clarity surrounding how teams within specific regions and agents as individuals are performing, we might consider SLIs and SLOs for incident management-related metrics. Let’s have a look at how the process works with some simple metrics that we can implement as indicators and objectives to contribute to a reduction in negative impact.

## SLIs for incident management metrics

In this section, we’ll construct a few journeys for one incident management persona, the **on-call agent**. Like the previous workshop chapters, in a workshop setting, the team would navigate through a discussion to determine the critical personas. These personas would range between on-call engineers, engineering teams, management, and other stakeholders of the incident management life cycle. In this instance, we will focus on the main persona, the on-call agent, establishing the various journeys that exist during an incident’s life cycle. The persona journey chart may consist of the journeys depicted in *Figure 13.1*.

![Figure 13.1 – Incident management persona journey for the On-Call Agent](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_13_1.jpg)<br>
**Figure 13.1 – Incident management persona journey for the On-Call Agent**

In this instance, the persona is the on-call agent, and the journey is associated with the respective metric they are expected to uphold versus a workflow against a specific system. The persona journey might begin with the following expectations for each of the metrics, outlined by their respective manager or as requested through channels up the management pipeline. *Figure 13.2*’s journey displays the expectations via the virtual whiteboard.

![Figure 13.2 – Incident management persona journey expectations](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_13_2.jpg)<br>
**Figure 13.2 – Incident management persona journey expectations**

However, we might need to define each journey and the expectations more in depth to better understand the requirements and determine whether the implemented platform for managing incidents maintains built-in features that auto-generate this information. For the sake of the text, we’ll pretend it does not. If it does not, how do we then aggregate available data in the current environment to provide similar information?

We might pose the following questions:

* Incident response occurs within 24 hours of creation with the response time being tracked as an SLO, where the SLO threshold is set to respond to a defined percentage of incidents within this time frame:
  + How is the workflow for response defined?
  + Can the current system automate this process?
  + Is there a logging or event system in place to retrieve timestamps?

* Incidents are resolved within 72 hours:
  + What is the definition of resolved with regard to the organization?
  + Is there a system event or resolution metric available within the current system?

* Incidents are acknowledged according to severity and prioritization criteria:
  + What is the current prioritization system in place?
  + Is there a metric or timestamp available that provides this information?
  + Is there an incident notification tool implemented currently?

* Incidents are detected within a certain time frame:
  + What are the available methods that detect incidents?

*Figure 13.3* depicts the updated whiteboard, which prompts the previous questions to the team.

![Figure 13.3 – Journey expectation requirements](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_13_3.jpg)<br>
Figure 13.3 – Journey expectation requirements

Let’s go over the requirements briefly.

### Incident response times

*Incident Response Time* focuses on the time it takes to respond to an incident. Due to the prior critical issues mentioned, the team might want to implement an initial response time of 24 hours. The incident response time is going to heavily depend on a few criteria:

* The size and structure of your current team:
  + Engineer headcount, location distribution, and so on

* The size of incoming requests and the current backlog:
  + Do your incoming requests exceed the ability of the agents?
  + Is the current backlog excessive with issues breaching various deadlines?

* The workflow used to report, triage, and assign a support ticket or internal incident:
  + Are current processes handled manually or with automation? Is there an opportunity to eliminate or reduce toil?

In addition to the preceding, your team may want to set response times based on severity, priority, or criticality, which is typically logged by the reporter at issue creation. Metrics considered at this stage might be the following:

* **Incident scale** – How widespread is the outage? Is every customer impacted or some percentage?
* **Degradation coverage** – What percentage of functionality is in a degraded state?
* **Time** – At what length of time (sometimes cadence) has the event taken place? Was the issue detected a day after it happened or immediately noticed?

You can accurately measure and report on incident response times if you take an accurate and thorough assessment of its current state. In other words, if your incoming requests exceed an amount that agents can effectively resolve within a given period, then it would work against the team and organization to set an SLO on **mean time to respond** (**MTTR**) for a given percentage of responses to meet an unrealistic expectation of a 1-hour response time.

Instead, you might consider staggering the response time according to severity and priority at a time length that is realistic and where customer complaints are minimal. This can be done asynchronously while using the off-call period to identify opportunities for automation and process improvement. In addition, it’s important to define what response time means. The quality of the response to the customer and the stance they take with your organization upon resolution greatly depends on the interactions made throughout the entire life cycle.

### Incident resolution times

The time it takes your team to resolve an issue, often tracked through the **mean time to resolution** (**MTTR**), is one of the most critical measurements during an active incident, with the resolution time being defined by an SLO that is monitored through SLIs. Many factors are at play when there is an ongoing issue, which can have a negative impact on customer satisfaction. Defining SLOs for incident resolution times is essential for ensuring timely and effective incident handling. Incident response teams can use these SLOs as benchmarks to guide their actions, streamline incident workflows, and expedite the resolution process. Often, this task is impacted by the following:

* The layers of communication that are required between the agent, customer, and other internal staff attempting to mitigate and resolve the issue
* The technical depth required to resolve the issue, including whether a code patch is needed or whether the problem stems from an upstream or third-party dependency

When considering or setting your incident resolution times, it’s best to consider the internal structure, processes implemented, and roadblockers to determine the best threshold or SLO.

### Time to acknowledge

Time to acknowledge focuses on how long it takes to notice and begin the work on a support request or internal incident. When considering customer success and responsiveness, acknowledgment of submission should happen as early in the process as possible, in addition to including as much information as possible. In today’s industry, this typically includes an automated process that is built into the respective platform or tooling of choice. Your acknowledgments will typically include the following:

* Identifier information regarding the request
* Information regarding the agent it’s been assigned to
* Expectations surrounding dialogue or communications related to the incident
* Additional information that is specific to your organization’s process

Each metric is important due to its impact on the customer experience and providing the on-call agent with the necessary insights related to how an incident is impacting the customer experience. This affords the opportunity to respond swiftly.

### Time to detection

Incident response, mitigation, and resolution are important phases within the life cycle. However, *when* the incident is detected plays a critical role in the success of each metric. If we regress to the concepts in [*Chapter 4*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/4), *Observability and Monitoring Are a Necessity and a Must*, SLO management enables SRE teams to measure customer experience with regard to the system versus a single metric such as CPU utilization, which is a metric internal engineers might pay attention to. Gauging when incidents are detected enables SREs to understand how effective the current monitoring thresholds are, with detection times being tracked as SLOs to evaluate the system’s effectiveness in meeting reliability goals.

For instance, if we expect an SLI to maintain a 99.5% SLO threshold over a 28-day rolling window, the inverse of the SLO (1-SLO) is the error budget or 0.5. This equates to approximately 43 minutes of downtime over a 28-day period before the issue is noticeable to the customer and begins to impact the experience. If we fail to implement fast-burn alerting and an incident occurs without detection, we’ve exhausted the budget prior to the end of the period. If early detection occurs, we can preserve the budget, leaving room for errors during code release or other system changes within the compliance period.

Defining specific SLIs and SLOs for on-call responsiveness and resolution is essential for measuring and improving incident handling processes. The earlier we can detect an incident, the more error budget we are able to preserve. Incorporating on-call engineers and agents into SLO frameworks is essential for maintaining service reliability and meeting customer expectations. When clear SLIs and SLOs are defined for on-call detection, responsiveness, and resolution, we can implement effective processes for on-call agents during live incidents and leverage the appropriate tools and technologies to do so. This helps organizations to ensure timely incident handling, minimize downtime, and deliver exceptional service to customers.

Incorporating on-call rotations and escalations into SLO frameworks requires careful planning and coordination. Linking SLO performance to incident management metrics such as **mean time to detect** (**MTTD**) and **mean time to resolve** (**MTTR**) will help you to provide valuable insights into the effectiveness of incident response efforts and SLO performance. When SLO performance is aligned with incident management metrics, this provides increased visibility of performance tracking among agents and trends through increased observability and enables the broader SRE org and stakeholders to make data-driven decisions, enhancing internal accountability and performance.

# Incident management and SLO preservation

Incident management plays an important role in the contributions toward ensuring the reliability and availability of systems and services. SLO preservation is achieved through incident management practices that ensure that system performance reliability is continuously monitored against SLO targets, with a focus on minimizing deviations from those targets during incident detection, response, and resolution. Preservation practices as related to your SLIs and SLOs in incident management can showcase their use as the leading indicators of potential issues and as targets for incident resolution. We’ll highlight examples of this in sections to follow. The goal of this book is not to cover topics such as artificial intelligence, automation, and machine learning; however, it’s important to consider the incorporation of their availability and the positive impact they can provide for SLO preservation.

Incident management is a critical component of reliability engineering and maintains the processes and procedures for detecting, responding to, and resolving incidents that impact system availability and performance. By establishing robust incident management practices, organizations can minimize downtime, mitigate risks, and maintain service reliability, ultimately enhancing customer satisfaction and trust. It also helps to create a collaborative culture where learning and continuous improvements occur in a systemic way while quantifying and measuring system performance for a customer-centric organization.

For the purposes of later discussions within this chapter, let’s revisit information related to an SLI created earlier in [*Chapter 8*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/8) for distributed systems. For more information related to the SLIs, please review [*Chapter 8*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/8). Highlighted here is information related to the *Cluster Dashboard* *Management* SLI:

* **Cluster Dashboard Management**: The web-based user interface is used as one of the several first lines of defense when attempting to debug live incidents. The deployment consists of a multi-container pod, deployed using the deployment object via Helm. Its workload is deployed into the system namespace. Therefore, there is a high expectation surrounding its availability:
  + **Error rate**: The workload needs to remain in a running state with a low error rate during normal operational conditions. Peak hours are critical.
  + **Pod startup**: During peak hours, the length of time it takes for a new pod to start and become ready to serve after failing over to another region is also critical.

Monitoring a dashboard may not immediately feel as if it is important to monitor. However, in an environment where there are numerous issues, customer complaints and peak hours vary within a region due to being globally distributed, and it’s important for the understaffed on-call team to have as much relevant information available to them when a new ticket is routed from a customer. This includes when an internal incident is escalated to the engineering teams. Therefore, the team thought it best to include a metric to report on its performance and determine whether an improved solution and/or method is needed.

Our goal in this section is how we can utilize SLOs for incident management to preserve the SLOs for our system environment, thus improving the experience of our customers after various complaints related to specific outages and incidents.

Let’s look at an example severity matrix for incidents. Although there is a correlation, this is not to be confused with a priority matrix, highlighting the impact on business importance. Severity focuses on the impact of an incident, such as a defect causing an entire application or platform to be down. *Table 13.1* depicts an example severity matrix (PagerDuty, 2021):

|  |  |  |
| --- | --- | --- |
| **Severity** | **Description** | **Response** |
| **Sev-1** | Critical issue requiring public notification and executives | Critical incident page (P1); notify internal stakeholders and the public |
| **Sev-2** | Critical issue impacting many customers’ ability to use the product | Critical incident page (P2) |
| Anything above this line is considered a “Major” incident | | |
| **Sev-3** | Minor customer-impacting issue requiring service owner attention | High-priority page to the service team |
| **Sev-4** | Minor issue, requiring action but not impacting customers’ ability to use the product | Low-priority page to the service team |
| **Sev-5** | Cosmetic issue but not impacting customers’ ability to use the product | Ticketing system |

Table 13.1 – Standard severity matrix

During incident response and resolution, SLIs and SLOs can serve as key benchmarks for evaluating the impact of incidents on service reliability. Incident management teams prioritize efforts based on the impact of SLI deviations relative to SLO targets, ensuring that service reliability aligns with customer expectations and minimizing downtime to stay within the defined error budget. This systematic approach ensures that incident response efforts are aligned with overarching reliability objectives, ultimately minimizing service disruptions and maximizing customer satisfaction.

Delays in identifying and addressing incidents can lead to increased and ongoing events surrounding downtime, increased customer dissatisfaction, and money left on the table, resulting in reputational damage. Therefore, organizations must prioritize proactive monitoring, rapid incident response, and efficient resolution processes to minimize the impact of incidents on service reliability.

In our instance, we want to implement SLOs to ensure our on-call agents and automated processes remain in compliance with the external commitments of our customers. To ensure that teams remain responsive to the issues that arise, we want to focus on the MTTD metric, calculated by dividing the total time between failures and detection by the total number of failures. It’s ideal to maintain an MTTD time as close to 0 as possible. However, it’s best to use internal data to determine percentiles for buckets to calculate starter SLOs. In the next section, we’ll calculate the MTTD and use this process as an example.

## MTTD SLO

The following is a table providing information related to incidents with logged failed MTTD measurements for our e-commerce platform during a month period within the **Eastern Standard** (**EST**) time zone.

|  |  |  |  |
| --- | --- | --- | --- |
| **Date** | **Incident Start** | **Detection Time** | **Minutes** **to Detect** |
| 01-03 | 11:30 a.m. | 12:00 p.m. | 30 |
| 01-10 | 3:30 p.m. | 3:47 p.m. | 17 |
| 01-12 | 12:05 a.m. | 12:22 a.m. | 17 |
| 01-14 | 8:52 p.m. | 9:35 p.m. | 43 |
| 01-20 | 11:10 p.m. | 11:16 p.m. | 6 |

Table 13.2 – Incident detection time for on-call agents over a month period

For this month, there were a total of 20 incidents, meeting a 1-minute detection time, with 5 exceeding the expected MTTD. On-call agents, over the last 28 days, experienced a total detection time of 128 minutes. If we divide this number by the total number of failures within the same period, we reach an MTTD of 6.4.

For statistics and probability to calculate starter SLOs, the recommendation is to read [*Chapter 9*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/9), *Probability and Statistics for SLIs and SLOs*, in Alex Hidalgo’s *Implementing Service Level Objectives* (Hidalgo, 2020). We’ve mentioned it’s best to keep the MTTD as close to 0 as possible, but on average, the MTTD remains within 1 minute using reliable automation. This month is much higher than normal due to recent code releases.

To calculate the error budget for a 95% SLO threshold over a 28-day period, we compute the allowable downtime by multiplying the total period (28 days) by the SLO threshold (0.95), leaving us with a 5% error budget. In this case, the error budget would be 1.4 days or 33.6 hours for the period, meaning that incidents exceeding this threshold would count against the error budget. This leaves 18.25 annum days or 1.5 days per month, or 1.5 hours per day.

In this instance, it is not as simple as considering availability and latency and analyzing logs and other metrics for data on requests to determine what your SLO ought to be. You can, however, look through your incident management platform statistics and gauge from available data what it ought to be to iterate until the appropriate value has been identified.

Another option is to use statistics, and probability formulas such as **expected value**, **median**, **maximum likelihood estimation** (**MLE**), or **maximum a posteriori** to better determine what your SLO ought to be. There are other formulas available, and each will be applicable to the type of dataset you are creating indicators and objectives for. In this example, measuring the MTTD for a period is not as concrete as we’d like it to be as the number of incidents in each period and failed detection times varies. However, we can infer what the effects of this number “x” may or may not be and treat detection times in terms of successes and failures. Of the 20 logged incidents for the month, 5 of them are failures due to exceeding the 1-minute expected detection time.

Depending on the observability tooling implemented, we might propose “x” as a metric that retrieves the number of incidents for a 28-day window and then calculates the MTTD by dividing the total detection time (based on timestamps) for each incident in the dataset, by the total detection time for every incident with a detection time > 1 minute. The MTTD for this month for the given dataset of 20 incidents is 6.4.

Looking at this from the perspective of good requests versus total requests would result in an SLO of 0.75 or 75%, violating the SLO threshold for the compliance period.

# Extending accountability to product engineering teams

When considering the impact of implementing SLIs and SLOs, it’s important to realize the aspect of extending accountability to the engineering teams building the product. Depending on the structure of the incident management practice within your organization, it’s likely that there is an extension that already exists to engineering teams at some touchpoint within the incident life cycle. The act of extending accountability within the product development life cycle and various workflows enhances collaboration between operations and product engineering teams by tying incident response to clearly defined SLIs and SLOs. This ensures that product engineering teams are not only responsible for building the product but also for maintaining its reliability and performance according to agreed-upon targets, improving both incident resolution and the overall product.

While operations teams traditionally handle incident response, product engineering has increased the level of visibility of system architecture, maintaining a direct impact on system performance, reliability, and availability. This level of visibility will also help SRE teams enhance the automation practices surrounding both incident management and DevOps tooling used to reliably deploy.

The incident creation flow typically starts with an end user in mind. As mentioned during the workshop chapters, this can vary from the typical user being a customer as a member of a customer account, an internal support agent, another internal SRE, or in a more elaborate organization a third-party tool creating an incident upon some notification. Regardless of who the incident is created by, it is created through some platform or your internal process and routed to the SRE on-call, as depicted in *Figure 13.4*.

![Figure 13.4 – Incident creation from user to on-call SRE](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_13_4.jpg)<br>
**Figure 13.4 – Incident creation from user to on-call SRE**

This flow should ideally align with the alerting mechanisms tied to your SLOs, ensuring that incidents are triggered only when performance deviates from the agreed-upon objectives. By using SLOs as a threshold for alerting, the process avoids interfering with other workflows while ensuring that incidents that affect service reliability are promptly addressed. To increase internal adoption with other internal engineers, my team found that incorporating current processes that aligned with workflows they were accustomed to increased buy-in among engineering teams and stakeholders. Ideally, if the SRE team is handling their SLOs via their own internal process, you’d want your incidents to flow as depicted in *Figure 13.5*, shifting to the product engineering teams.

![Figure 13.5 – SRE team shifts SLO violation or incident to the product engineering team with relevant information](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_13_5.jpg)<br>
**Figure 13.5 – SRE team shifts SLO violation or incident to the product engineering team with relevant information**

Incorporating incident management responsibilities into product development life cycles and workflows ensures that reliability considerations are integrated into every stage of product development. Product engineering teams should prioritize reliability requirements, conduct thorough testing and validation, and implement proactive monitoring and alerting mechanisms to detect and address potential issues early in the development process. By embedding incident management responsibilities into product development workflows, organizations can minimize the risk of reliability-related incidents and deliver more robust and resilient products to customers.

Collaboration between operations and product engineering teams is essential for improving incident response and resolution. Operations teams provide valuable insights into system performance and reliability, while product engineering teams offer expertise in identifying and addressing underlying issues. By fostering open communication, sharing knowledge and best practices, and establishing cross-functional incident response teams, organizations can streamline incident management processes, accelerate incident resolution, and drive continuous improvement in reliability and service quality.

# Summary

By integrating SLIs and SLOs into the incident management process and extending accountability to your internal on-call and product engineering teams, organizations can elevate the role of incident management in reliability engineering. Along with defining clear and measurable SLOs, incorporating on-call engineers and agents into SLO frameworks, and extending accountability to product engineering teams, organizations can enhance their ability to respond to and resolve incidents effectively, ultimately improving the reliability and availability of their systems and services.

Although it is not a core aspect of this chapter or even the book, it’s important to understand that the structure of the practice internal to your organization plays a key role in your ability to scale or extend accountability. The more hands on deck, the bigger the increase in the necessity of structured and aligned practice. Continuous monitoring, measurement, and refinement of SLIs and SLOs in partnership with incident management will enable your organization to drive improvements through incident management practices. In the chapter to follow, we will touch on making decisions with the information at hand.

# Further reading

Here, you can review and read the referenced articles and books for additional reading about concepts mentioned in this book:

* Hidalgo, A. (2020). *Implementing Service Level Objectives*. Sebastopol: O’Reilly Media Inc.
* PagerDuty. (2021, May 18). *Severity Levels*. Retrieved from <https://response.pagerduty.com/before/severity_levels/>.
* Team Asana. (2024, Feb 16). *What is Incident Management? Steps, tips and best practices*. Retrieved from <https://asana.com/resources/incident-management>.
