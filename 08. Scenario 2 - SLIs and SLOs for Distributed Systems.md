# Scenario 2: SLIs and SLOs for Distributed Systems

In previous chapters, we covered the history of reliability engineering, SLIs and SLOs, and the importance of establishing a reliability engineering team for your SLO process. In this chapter, we plan to shift the focus toward building SLIs and SLOs for distributed environments. When SLIs and SLOs are mentioned, the conversation typically shifts toward applications. This may steer the conversation in the direction of topics surrounding client requests via HTTP request and response, latency for page loads, or overall application availability and uptime. In distributed environments, it is equally important to consider the following when reporting on application availability:

* How the underlying infrastructure can impact application availability and uptime, with an emphasis on how control over infrastructure differs between on-premises and cloud environments. In cloud environments, the provider typically manages the infrastructure, which influences the performance of SLIs and SLOs. For on-premises infrastructure, organizations have full control, allowing them to implement tailored solutions that affect the performance and reliability SLIs.
* The difference between reporting on SLI and SLO metrics for infrastructure hosted on-premises versus with a cloud provider.
* The impact of networking on data processing; thus, impacting storage metrics and CRUD database operations and performance.
* Consideration of how multi-region deployments and load balancing impact latency, availability, and reliability SLOs. Load balancing between regions can optimize traffic routing, reducing latency and ensuring high availability, but also needs to be measured by appropriate SLIs (such as response time and error rates) to align with the SLOs for system reliability.

There are numerous other considerations that we should consider that may not immediately come to mind. However, it’s important to continuously structure the conversation in a way that encourages the team to build performance measurements and thresholds for various tasks and activities used within the system and daily operations.

This chapter uses the same steps outlined in *Chapters 6* and *7*, which include the following:

* Understanding the application architecture
* Identifying personas and the persona journey
* Establishing application (system) boundaries
* Specifying SLI types based on the identified system boundaries
* Implementing SLIs with accurate touchpoints based on the system specifications
* Understanding required considerations when prioritizing SLIs and SLOs

So, let’s get started!

# Understanding the application architecture

For this chapter, we’ll extract the specific components and shift our focus toward SLIs and SLOs for the distributed parts of the system architecture. The architecture is depicted in *Figure 8.1*.

![Figure 8.1 – Application and infrastructure reference (also depicted in previous chapters)](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_1.jpg)<br>
Figure 8.1 – Application and infrastructure reference (also depicted in previous chapters)

If further details for the application and infrastructure architecture depicted in *Figure 8.1* are needed, please reference the *Understanding the application architecture* section of [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7).

[*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7) focuses on the application layer components while this chapter focuses on the distributed components of the cluster, which include some of the cluster components responsible for orchestration. The goal is to highlight how the deployed services can play a role in your application meeting external agreements or SLAs. Therefore, the application services are not mentioned here but expect the chapter to follow a similar flow.

The cluster depicted in *Figure 8.1* also includes a load balancer within each region to route traffic or requests to the appropriate service. The external storage provider is configured using the storage class CRD and is the storage provider for persistent volumes and volume claims. More information on the latter can be found in the Kubernetes website documentation linked in the *Further* *reading* section.

Now that we have reviewed the architecture at a high level, let’s understand which personas play a key role in our distributed environments. At any point, please review [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7)’s application architecture to understand the role of a specific component in the text.

# Identifying personas and the persona journey

In a normal workshop, the conversation would start after an assessment of internal data and team members to understand the various personas. This includes a ranking system to follow to establish which personas are most critical to start. For this chapter, we will begin the conversation of identifying and defining the persona journey by focusing on specific personas. In this distributed environment scenario, each persona plays an integral role in developing, configuring, or managing the components that make up the infrastructure platform that hosts the application services. These personas are as follows:

* **Cluster administrator**: Responsible for ensuring the cluster is available within defined SLOs and configuring security to meet security and performance SLIs. Availability targets should be defined in terms of uptime, such as 99.9% or 99.99%, with measurable metrics for system reliability (e.g., node uptime, cluster health status).
* **Developer**: Responsible for deploying applications into the cluster, ensuring deployments meet established SLOs for availability, response time, and reliability. Developers should monitor and optimize SLIs such as deployment success rate, time to deployment, and service response time, to ensure that the application performs within defined reliability targets.
* **DevOps tooling**: Operational tooling that integrates with the cluster to improve developer productivity and cluster maintenance.

Each persona might appear on our whiteboard, as follows in *Figure 8.2*:

![Figure 8.2 – Personas identified by the team](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_2.jpg)<br>
Figure 8.2 – Personas identified by the team

It’s important to highlight that the DevOps tooling persona is a bit too broad. In a workshop setting, we’d want to investigate the importance of this persona and narrow it down to a specific tooling. We’d then highlight the significance of needing to measure the tools, possibly based on some interactions with our application. Based on previous experience, there has been mention of tooling such as CI/CD pipelines and credential management tooling. It’s mentioned here to serve as a reminder of the various integrations from internal tooling to third-party integrations and other APIs that can and should also be included in the team brainstorming or analysis sessions. This helps to identify gaps that impact performance measurement but may immediately appear as the offender of performance degradation as it relates to the application service.

Moving forward, let’s imagine the cluster administrator and developer personas are the two personas we need to focus on to improve infrastructure immediately. In addition, we know first-hand that there have been numerous reported issues regarding the underlying infrastructure impacting application availability and performance. Therefore, the team decides it is best to focus on both and generate as many journeys as possible to gain better insights and immediately make technical decisions. The engineers have enough experience with the platform and customer-facing outages to immediately begin gauging performance and identify short-term wins.

We then decide to move the selected personas over to the persona journey section of the virtual whiteboard. In a live workshop, the team or individual(s) running the workshop would have likely established a ranking system to eliminate bias and shift the correct personas to the next stage. See *Figure 8.3*.

![Figure 8.3 – Distributed systems persona and journey template](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_3.jpg)<br>
**Figure 8.3 – Distributed systems persona and journey template**

We then want to collaboratively work together to determine the various journeys each persona utilizes when trying to achieve some outcome in the system. If the team has developed a backlog of internal data from various sources, this can be used to make assumptions. This includes monitoring systems and other dashboards or charts that are currently configured. However, let’s start by assuming we need to take the following flows into consideration:

* Applications that the respective persona deploys or manages within the cluster.
* Tasks completed to manage the cluster and respective configuration settings.
* Resource consumption measurements that the respective persona measures and monitors.
* Native objects created, deleted, or updated by the respective persona. This includes cluster and infrastructure components.

Based on discussion among the team, we determine each role is responsible for the following journeys to complete the tasks previously mentioned:

* **Cluster administrator**:
  + CRUD operations
  + Node addition/deletion
  + Cluster IAM access
  + Storage utilization
  + Cluster resource utilization
* **Cluster developer**:
  + Incident management
  + Application deployment
  + Namespace resource utilization management
  + Application resource scaling
  + Ingress management
  + Dashboard management

We might then update the whiteboard to include the following journeys in alignment with the respective persona. The information depicted in *Figure 8.4* represents the persona journey and should be established from the internal data and metrics in place with the organization and developed by team members.

![Figure 8.4 – Distributed systems persona journeys](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_4.jpg)<br>
**Figure 8.4 – Distributed systems persona journeys**

After further discussion, we might then have the team select a few journeys that are critical or great candidates to start with. In *Figure 8.4*, critical journeys are marked with a dot on their sticky note. In this scenario, we will select four journeys that we believe are critical to start based on an assessment of internal data. For each journey, we might pose additional questions to better understand what we are trying to capture and measure for each journey. It’s important to probe for additional information through dialogue to better understand the requirements. For the marked journeys, we might pose the following questions and information for additional clarity:

* **Node addition/deletion**:
  + The workflows for adding and deleting a node should exist independently as they are on opposing ends of the spectrum.
  + For each operation, there should be a clear SLO for latency and success rate. For example, a node addition should have an SLO for how quickly the node reaches a “ready” state (e.g., within 5 minutes).
  + Similarly, for node deletion, the SLO should define how quickly workloads are rescheduled without downtime, and ensure that all workloads are relocated successfully, meeting a defined success rate (e.g., 99.9% success rate).

* **Cluster** **resource utilization**:
  + What does *manage* mean in the sense of this journey?
  + Is this a cluster- or namespace-scoped resource? Are resource quotas an option?
  + Are there specific resources we are gauging? Databases deployed via statefulsets? Or standard deployments?
  + Of those resources, has either been involved in an excessive number of incidents?
  + Network throttling and node outages due to resource consumption.
  + Has either caused friction in the customer experience to an unacceptable degree?

* **Application** **resource scaling**:
  + What encourages the team to focus on “scale”? Has there been an excessive number of outages in a specific time zone? At certain hours?
  + Is there a decrease in performance or website accessibility when there are a certain number of replicas for the respective application workload?

* **Cluster** **dashboard management**:
  + What does *maintain* in this instance mean?
    - Does it relate to the charts and dashboards?
    - Is the deployment via automation and YAML?
  + Is there a specific deployment or resource that the team can monitor?
  + When is this deployment most critical for the targeted persona?

Upon further discussion, the team may decide on the identified journeys and raise additional questions. They may even identify certain journeys that should function independently, such as node addition and deletion. Although similar but opposing transactions, there are different tasks required to complete both. Some journeys may function independently to better track and reflect the value they bring to the organization and customer experience. We might restructure the previous journeys and end with the following journeys, which include customer expectations, requirements, or indicators the journey was successful. The restructured journeys appear as follows:

* **Node addition**:
  + Node displays a “ready” state.
  + Node accepts new workloads.

* **Node deletion**:
  + Node no longer displays in cluster output.
  + Node workloads have been rescheduled.

* **Application** **resource scaling**:
  + Deployments scale when performance accessibility decreases:
    - Network throttling.
    - HPA is already deployed.

* **Cluster** **dashboard management**:
  + Deployed dashboard is readily available:
    - *Readily available* is determined by the respective pod being in a healthy and running state.

* **Cluster** **memory utilization**:
  + A percentage of the memory needs to remain available for application scaling and rescheduling.
  + Resource utilization metrics are available in the cluster node output.

* **Cluster** **CPU utilization**:
  + A percentage of the CPU needs to remain available for application scaling and rescheduling.
  + Resource utilization metrics are available in the cluster node output.

If we regress back to the journey phase map, we might also walk through each of the journeys and develop the following artifact for each identified journey to better outline the process. For the sake of the text, we will utilize a single example, as depicted in *Figure 8.5*.

![Figure 8.5 – Node addition journey for the cluster admin persona with kubeadm installation](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_5.jpg)<br>
**Figure 8.5 – Node addition journey for the cluster admin persona with kubeadm installation**

In addition, it’s important to mention again that the team can start with the most critical personas and journeys. If a persona or persona journey is not completed in the first run, it is best to iterate through the process again to identify the next group. In the iteration to follow, there will not exist a need to identify personas and journeys again. You’ll hear this restated in each of the chapters in this part of the book as a gentle reminder. As it relates to the text, we are going to keep moving forward! Our team has settled on the journeys and expectations previously mentioned! It is time to understand the technical implications of the workflow.

# Establishing application (system) boundaries

Now that we have identified the critical personas and their respective journeys, it is time to understand a bit more about how this translates to the technical aspects of the distributed environment. How do the components interact with one another to achieve an outcome within the system to help developers achieve their respective goals? At what touchpoints do we collect metrics? How do we aggregate these metrics in a way that reflects our SLIs and enables the team to determine whether we are meeting our SLOs? This helps to quickly assess whether we are meeting the agreed-upon service levels (SLAs) and whether corrective action is required. This is ultimately the sort of thinking as dialogue continues during the workshop.

We can utilize the customer journey map from *Figure 8.5* to assist with creating a visual diagram of the interactions between the technical components, to clearly understand how the components interact with one another and where. This step within the process can be accomplished through collaborative talking points, but occasionally, it will help to visualize the process and ensure that all individuals are on the same page and agree. We should also take this into consideration when trying to consider individuals who maintain different learning styles. The interpretation of what is discussed can be left to the imagination only if you allow it to be. We might utilize a sequence diagram to visualize the technical flow of interactions while ensuring that key touchpoints are aligned with relevant SLIs, such as request latency, error rates, or availability, which directly contribute to SLOs, as depicted in *Figure 8.6*.

This does not indicate that this is the best diagram or the recommended approach. Experience has shown that talking through workflows has worked. Sketching system architecture through talking points has also worked. Many organizations may already maintain in-depth design documentation to pair with the workshop. The best approach is the one that makes sense and should be considered in the early stages of developing or scheduling your workshop. Let’s look at the interactions displayed in *Figure 8.6*.

![Figure 8.6 – Node addition sequence diagram](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_6.jpg)<br>
**Figure 8.6 – Node addition sequence diagram**

*Figure 8.6* displays the various components from the container orchestrator deployed within the cluster. The orchestrator is responsible for scheduling and running the various services deployed to the cluster, ensuring that workloads remain in a healthy state and consistent with the desired state configured in the resource’s deployment YAML file.

Within the interaction, we have touchpoints that begin with the customer utilizing the kubeadm bootstrapping tooling for cluster deployment and node join management. In addition to the networking communications that happen with the API server, a Kubernetes component and kubelet resource are deployed on all nodes within the cluster. Behind the scenes, various tasks occur to add and remove a node to and from a cluster, respectively. The sequence diagram helps to visualize the touchpoint to better understand where we may need to measure what.

When approaching this step, the goal of the discussion is to dissect at a lower level what takes place on the backend to better understand the metrics you want to collect, metrics you need to collect but that may not be established, and what and how different or several components work together to achieve the end goal. If your organization has an internal architecture or design team or is evaluating software and technology from another organization, much of this information is likely readily available to you.

If we redirect to the *application resource scaling* persona journey, we can dive a bit deeper into the journey to better understand the technical components and architecture.

However, upon further review, we realize the team mentions it has a solution deployed within the cluster that it has not fully implemented yet. It is the HPA that is native to the Kubernetes orchestration layer. The HPA automatically scales a workload based on resource utilization metrics (such as CPU or memory usage) to meet the desired SLOs, ensuring that response times and availability are maintained under increasing traffic. These metrics are gathered through the metrics API and should be monitored to ensure they align with our established SLIs for optimal service performance. If our application experiences an increase in traffic during the holidays, we might want to add additional resources to the cluster, with which we can balance the load.

Initially, we listed this journey in reference to the developer persona. If we think of the scaling process done by a developer, it might happen through a deployment file, some script, or a third-party integration deployed into the cluster or local infrastructure code base repository. It’s important to differentiate between the persona being a tool, API, or some integration versus being a human role. It will help the team direct the conversation in the right way when we differentiate between the types. For the sake of the text, we will leave it categorized the way it is.

Therefore, our customer journey map might appear as follows for HPA, and with the developer persona, since they are the ones implementing or configuring the functionality:

![Figure 8.7 – Resource autoscaling journey](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_7.jpg)<br>
**Figure 8.7 – Resource autoscaling journey**

We might then determine that our journey ended up being too process-oriented and want to put together a quick sketch of the components that monitor, notify, and scale workloads to better understand what component logs we need to search, what metrics are currently available via `metrics.k8s.io`, and what metrics are reported to the server running in our cluster.

At a high level, the communication channel begins with a request sent from an external client. The request is routed to and through the API gateway for request filtering and authentication and is then routed to the load balancers, deployed on their own nodes within the cluster. The load balancers serve as a mechanism to route traffic to the appropriate backend service using ingress capabilities. Once the configured details have been reviewed, the request is routed to the backend service, which, in this instance, is our inventory service to retrieve information regarding a specific product from the inventory database. You can review additional details in *Figure 8.8*:

![Figure 8.8 – Client-to-service and HPA communication flow](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_8.jpg)<br>
**Figure 8.8 – Client-to-service and HPA communication flow**

Asynchronously, the HPA service, which is deployed within the same region and on the same node, runs through a loop at an interval set by the development team to query the resource utilization metrics within the HPA definition file. It then identifies the respective objects and retrieves the appropriate metrics from the metrics API to decide whether it needs to scale resources up or down.

Because we had our original design documents available, we decided that we no longer needed to go through the process of creating a sequence diagram to loop through the communications between each object.

What we have found is the HPA functionality enables the team to set configurations such as the following:

* `--horizontal-pod-autoscaler-sync-period` to pass the time interval that the team would like for the controller to query resource utilization for the respective objects.

* HPA uses the metrics server. The metrics server retrieves its data from kubelet and exposes it in the API server through the metrics API. This means there are no additional configurations or deployments in the cluster:
  + `Metrics.k8s.i`

* It also handles aggregated APIs, which enables us to implement external and custom metrics:
  + `Custom.metrics.k8s.io`
  + `External.metrics.k8s.io`

With that being said, the team decides it best to move forward with both journeys mentioned during the mapping process as well as an additional journey that is a bit simpler to implement.

# Specifying SLI types based on the identified system boundaries

Before we move on to specification, in [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7), we outlined the SLI types. If a refresher is needed, please review [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7)’s *SLI specification* section for reference. For the context of the chapter, it is simply used to remind us of the types available before we begin the brainstorming session. As your team and organization get used to the process, they will develop a natural ability to immediately identify the appropriate types during discussion.

As a refresher, the persona journeys that we have agreed to focus on SLI development for are referenced in the following list. For the purposes of this section, we will not provide or go over definitions or descriptions. Each of the aforementioned is available for review in [*Chapter* *7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7)’s text:

* Application resource autoscaling
* Cluster resource utilization:
  + CPU
  + Memory
* Node addition
* Cluster dashboard management

It’s important to be mindful of the journeys that you attempt to implement SLIs for. It impairs the group’s ability to focus on the quality of specification and implementation. In addition, if there are any blockers or too many unknowns, it can cause the team to shift from one SLI to another, and on to another, without giving much attention to the end-to-end process and ensuring the end goal is achieved.

In this instance, there is an overlap between the resource utilization journey and the autoscaling journey. It was thought best to add the CPU and memory requests under the umbrella of the autoscaling journey. In addition, CPU and memory utilization metrics should be considered as part of the system’s reliability measurements (SLIs) that affect the end user’s experience. These should not only be monitored but also mapped to the SLOs to measure the ability of the system to meet a customer’s needs, such as response time or availability under load. While resource utilization can indicate system health, it should be linked to the desired SLOs rather than treated as isolated operational metrics. Considering our persona in this instance is our developer managing the infrastructure and deploying the application workloads, we are going to keep it in the conversation and see whether anything comes of it. Here are some questions that we may want to keep top of mind:

* Can we incorporate each of these into the HPA journey and, by default, the SLI?
* Can we structure this in a way that internal engineers and developers are the customer persona, focusing on their experience?

This may not be the ideal method for solving this problem. Please do remember that when you and your team are running your workshops, it is ultimately up to you to do what is best for all parties, including stakeholders, that are involved. With that being said, let’s do some initial brainstorming to set a better stage for what we need to measure:

* **Application resource autoscaling**: The team would like to measure the performance of autoscaling capabilities in a way that lets the dev team know the customer is benefiting from this. Based on the metrics design process review, we find much of the information and metrics are logged via the kubelet and timestamps are available in component logging:
  + **Throughput**: The team would like to capture the requests at some rate and during some given period.
  + **CPU resource utilization**: It is important that CPU utilization limits across nodes do not exceed a certain percentage. This may require additional configurations at the namespace or pod resource levels.
  + **Memory resource utilization**: It is important for memory utilization to not exceed certain limits. This may require resource limit configuration at the namespace, pod, or node levels.

* **Node addition**: This is important to the team, due to scaling nodes up and down frequently, including during peak hours. The team cannot afford a cluster that is too unhealthy to adapt to the frequently changing demands with prolonged delays in scaling operations:
  + **Timed-event**: The length of time it takes to complete the additional operations, from initiation to readiness.
  + **Availability**: The cluster should remain in a ready state, to accept new workloads and manage current workloads during node operations.

* **Cluster dashboard management**: The web-based user interface is used as one of the several first lines of defense when attempting to debug live incidents. The deployment consists of a multi-container deployment via Helm installation and deploys the workloads into the system namespace. Therefore, we can look toward the deployments remaining available:
  + **Error rate**: Maintain a specific error rate during normal operational conditions.
  + **Pod startup**: The time it takes for a new pod to start and become ready to serve.

An SLI specification flow is depicted in *Figure 8.9*.

![Figure 8.9 – Cluster admin and developer specifications](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_9.jpg)<br>
Figure 8.9 – Cluster admin and developer specifications

Now that we have an understanding of what we want to measure, it is imperative that we determine the following:

* Where current configurations allow us to measure the “what”
* What configurations are in the current environment that help to capture the limits of the “what”:

  + Event logging
  + System and workload logging at the cluster level
  + Probing for additional data
  + Implementing a new observability layer

In addition to answering the preceding questions, we also want to look at internal data to begin to understand what the limits or thresholds are. For instance, if we are calculating the error rate remaining below a specific threshold during normal operating conditions, we may want to first define what normal operating conditions are and then look to the data to answer related questions.

If normal operating hours are Monday to Friday 8 a.m. to 4 p.m. EST, we should define the expected SLOs during these hours, such as availability or response time. Once SLOs are defined, we can analyze these hours. For example, we might look for peak hours that could indicate a higher-than-normal load and whether the system can handle this load without breaching the availability or latency SLOs. These are the questions we want to investigate using current and historical data. Much of this information may not be available at this stage but it is important to consider. The data investigated at this stage will inform the team of the metrics and components that need to be considered during the implementation stage. With that being said, let’s shift our direction to how we measure the “what.”

# Implementing SLIs with accurate touchpoints based on the system specifications

Once we specify our SLIs, we want to shift the conversation toward the technical implementations, which we discussed in the previous two sections of this chapter.

The more dialogue that happens between team members and stakeholders in the beginning stages, the easier the conversation and implementation is in the later stages due to the data having already been gathered. This is under the assumption that the respective architectural or design documentation has already been reviewed and analyzed by the enablement team.

Now, how do we transition the narrative from specification to implementation? As we did in the prior chapter, the goal of this section is to redirect the conversation toward “how” and “how much.” *Figure 8.10* displays the high-level shift from specification to implementation.

![Figure 8.10 – Cluster admin and developer specifications](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_10.jpg)<br>
**Figure 8.10 – Cluster admin and developer specifications**

The structure of each persona journey, starting from defining the SLI type, shifting toward SLI expectations to specifications, and finishing with SLI implementation, should also naturally guide us toward defining the corresponding SLO. The SLO defines the target or goal for the SLI, ensuring that the SLI is not only measured but also aligned with the desired level of service that reflects the customer experience or business needs. In some instances, the data available will consist of enough information during earlier phases. In other instances, it may take a lot of dialogue between the team in phases to determine the appropriate SLO.

If your data does not immediately indicate a clear pattern, it’s often useful to start with an agreed-upon initial SLO based on team consensus, or historical performance data. From there, monitor the system’s performance over a defined period (cadence), adjusting the SLO as more data becomes available. This iterative process ensures that the SLO is both realistic and aligned with customer expectations and service reliability. The length of cadence greatly depends on the criticality of the SLO development and the need to report on it.

For our SLIs, we understand we are capturing service performance for workflows and resources within the cluster or infrastructure. SLI and SLO implementation focuses on which components and existing metrics within a system are best to use and measure a specific user journey. In this instance, we are focused on the measurement of throughput, availability, and error rate. For this section, we’ll focus on the node addition SLI and SLO, which is as follows:

* 90% of node addition operations are completed within 15 minutes.

  This is indicated by the node status reporting `Ready` and the node `StartTime` reporting a time within the last `x` minutes.

In an earlier section, adding a node to a cluster is triggered by an action completed by the developer. This results in an event between the client and API server, of which a timestamped event populates in the logs. The timestamped event ends with confirmation information being sent from the local kubelet resource to the API server and then to kubeadm. For an end-to-end measurement of the node, the closing timestamp should post a time less than or equal to 15 minutes after the initial timestamp triggering the workflow. Through our observability tooling of choice, we will be able to query and populate the length of time to configure an SLI and SLO.

It is important to understand the architecture of the system you are measuring to formulate these decisions. For our SLI, we want to track the ability to spin up a node, and the node reports a `Ready` state within a certain amount of time, to indicate we can immediately deploy workloads to it. We can use a more reliable metric, such as `kubelet_node_startup_duration_seconds`, which reports the node startup time in total, or the `kubelet_node_startup_post_registration_duration_seconds` metric, reporting the node startup time post-registration activities. Registration, as it relates to Kubernetes, provides the API server with metadata about the node, so workloads are scheduled to it accordingly. In this scenario, post-registration communication is ideal, as we want to track the readiness to accept workloads versus merely being available.

In this instance, we can utilize a readily available metric to calculate a node’s readiness to accept workloads and we would want to kickstart the SLI and SLO through it. In other scenarios where a metric is not readily available within the system, we would capture the logs available from the mentioned components to build a single metric. Thus far, we know that `kubelet_node_startup_post_registration_duration_seconds` should remain less than or equal to 15 minutes.

# Understanding required considerations when prioritizing SLIs and SLOs

The ranking system is based on internal dialogue between the individuals leading the initiative and the technical staff responsible for the development and management of the technical components or designs. Like the previous chapters, and likely to be mentioned in the following chapters, the prioritization phase should occur between the decision-making teams and through natural conversation and communication with relevant stakeholders. In our instance, we might consider the following assumptions:

* Resource scaling during peak hours, especially the holidays, has a larger impact on the number of sales completed, number of carts abandoned, and number of customers that redirect to a competitor due to poor performance.
* Node additions, completed within a certain timeframe, impact business metrics in the long term. Although feasible, it is not as critical to business as ensuring scalability during peak business hours.
* For our third SLI, it is feasible and critical to the teams handling on-call responsibilities, thus having a business impact. However, its impact is not directly tied to business, as with the initial two SLIs, so we will rank it as a lower priority and consider it to be associated with ongoing maintenance.

![Figure 8.11 – SLI prioritization based on business impact and feasibility](https://static.packt-cdn.com/products/9781835889381/graphics/image/B22155_08_11.jpg)<br>
**Figure 8.11 – SLI prioritization based on business impact and feasibility**

Prioritization is and always should be a collaborative effort, while the processes and technical analysis aspects help to structure and shift the prioritization phase in the most accurate direction. When prioritizing your SLIs and SLOs, it’s important to focus on feasibility, as it relates to your engineering teams and business impact, broader organizational goals, and contributing to meeting strategic goals and enabling other initiatives. As a reminder, be sure to include relevant stakeholders in the discussion, as they are likely to understand the internal business impact from a broader perspective.

# Summary

When implementing SLIs and SLOs, it is common to immediately shift toward thinking about applications and the typical metric measurements that you’d expect to see from a frontend application. This statement is opinionated based on discussions and publicly available content. However, I hope that this chapter not only reinforced the steps or starting points for building and running your workshops but also highlighted the importance of measuring the components’ performance in your distributed environment, including the infrastructure your applications run on. If a measurement or metric does not exist, it’s imperative to consider the abilities of observability solutions to create and implement the necessary metrics and queries to quantify the customer experience and service performance.

Another thing that is not immediately mentioned throughout but should be considered is how the underlying infrastructure plays a critical role in the performance of your services. This especially includes when an organization is deploying their workloads with a cloud provider who can only commit to a specific level of service already defined within their SLAs, as well as application integrations from third-party providers that may have some dependency. It is critical to not over-commit to a level of performance beyond what the provider can commit to. These are also discussions that should take place as a part of your workshops.

How do we inject these dependencies into our own indicator measurements? Is it appropriate to simply commit to a level of performance that replicates their SLAs? Let’s have a look at that in the upcoming chapter.

# Further reading

You can review and read the referenced articles and books for additional reading about concepts mentioned in this book:

* *Kubernetes* *documentation*: <https://kubernetes.io/docs/concepts/overview/components/>.
* *gRPC status* *codes*: <https://grpc.github.io/grpc/core/md_doc_statuscodes.html>.
* *Custom Backend* *Metrics*: <https://grpc.io/docs/guides/custom-backend-metrics/>.
