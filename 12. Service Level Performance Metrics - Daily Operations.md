# Service Level Performance Metrics: Daily Operations

In previous chapters, we discussed the history and importance of reliability engineering. We also navigated through examples of an SLI and SLO workshop for various aspects of your system environment, highlighting additional system features that your organization may want to raise awareness about. However, once SLIs and SLOs are implemented, what does the team do daily to iterate and improve upon them? How does the team maintain the necessary mindset to keep building upon their SLIs and SLOs in a way that is intuitive and cross-functional? In my previous experience, one of the major shifts was maintenance in a cross-functional manner while attempting to advocate for indicators and objectives.

In this chapter, you can expect the discussion to center around the iterative nature of SLIs and SLOs. The goal is to improve your understanding of how your organization can leverage various practices and processes to continuously refine and improve your reliability and performance metrics. It will also highlight the iterative approaches available for refining SLIs and SLOs, highlighting the importance of rapid iteration in response to changing business needs, user feedback, and technical constraints while adjusting over time.

To experience the short- and long-term benefits of customer-centric performance monitoring, we’ll need to focus on the following:

> * The appropriate approaches to leverage for iterations and adjustments
> * How to create iterative processes and gain cross-functional operational perspectives
> * How to improve responsiveness and enhance the organization’s effectiveness

Managing your SLOs with high efficiency and effectiveness will require consistent effort and attention. However, you want to ensure that you don’t fall into the cyclic pattern of managing your SLOs in the way that you would handle support requests or incidents, although the practice of each should integrate or flow into your SLO workflow. Some areas that we will cover within this chapter are the following:

> * Understanding the iterative nature of SLIs and SLOs
> * Monitoring for your SLIs and SLOs
> * SLI and SLO iteration and continuous improvement
> * Adapting to SLI and SLO changes
> * Cross-functional collaboration for SLIs and SLOs
> * SLI and SLO performance reviews

Before we dive into the previously mentioned concepts, it’s important to understand that the current state of your organization and team is going to differ from other organizations, resulting in a different set of needs and wants. Therefore, it’s important to take your current organizational structure and internal processes into consideration while reading. This will help you to gauge and compare what needs to be changed, removed, or added to bring your organization or team to the desired state.

# Understanding the iterative nature of SLIs and SLOs

When considering the rapid development of software and technology in today’s world, practices such as reliability engineering have become increasingly important. Businesses will need approaches to increase the speed of the delivery of software, reduce the amount of customer-impacting incidents, and improve the efficiency of the processes and tooling their development and engineering teams use to deliver software.

Even as a business grows and adapts to these advancements, there will always exist new integrations and feature requests from customers requiring additional adjustments. This increases the need for customer-facing staff to understand the iterative nature of concepts such as SLIs and SLOs to aid with quantifying how customer-centric services and systems are. Through iteration, teams can incorporate lessons learned from past experiences, address emerging issues, and adjust SLIs and SLOs to better align with organizational goals and user expectations. By embracing an iterative approach, organizations can build agility, resiliency, and responsiveness into the dynamic nature of modern software systems.

# Monitoring your SLIs and SLOs

In the previous chapter, we discussed monitoring and alerting configuration options available for monitoring the customer-centric features of your system. The end goal was to transition the focus from consumption-based resource monitoring to customer-centric resource monitoring. However, your team still maintains the responsibility of managing the processes used to improve your SLI and SLO life cycle. This is especially important if you expect to offboard your process to another team. If your team opts to build internal tooling versus investing in a platform dedicated to managing your SLOs and metrics, this can end up requiring increased effort and dedication from your team(s) and organizations.

SLIs and SLOs enable us to regularly monitor and track system performance and reliability, but how do we then increase the reliability of the processes we use to deploy and maintain? You can expect to see this discussed in more depth in the sections to follow. Teams should ideally utilize monitoring tools and dashboards to collect real-time data on each of the SLIs and their respective SLOs, ensuring that the monitoring is tailored to the agreed service reliability objectives, such as availability, performance, and latency. The same level of sensitivity used to secure processes that focus on monitoring security features and data traffic through automation pipelines and internal databases should be used to ensure compliance with relevant security standards.

# SLI and SLO iteration and continuous improvement

When considering ongoing monitoring for continuous iteration and improving upon your SLIs and SLOs, you want to “treat your SLOs as code.” This suggests handling the SLI and SLO process with similar processes and tooling that you would use to manage the source code for applications and services. By treating SLIs and SLOs like code, teams can leverage familiar concepts and tools, making the process smoother for internal engineering teams and enhancing the overall efficiency of service-level management. If you plan to potentially offboard your process to another team, this will greatly simplify doing so. On a team where you are deploying and managing software code for your system and services, your team is likely doing the following:

> * Developing SLIs and SLOs with distributed teams (in some cases).
> * Using a version-controlled system for version control and code management.
> * Building and testing code in a development, staging, or testing environment.
> * Handling various dependencies in preparation for packaging your code for deployment.
> * Utilizing various security mechanisms for deployment to a production server.
> * Monitoring the software code and validating the success of its deployment.
> * Have implemented a process for rolling back code changes.
> * Have CI/CD pipelines that streamline all the preceding steps for you.

For SLI and SLO development, it is important to follow a structured process to define SLIs, establish SLOs, and implement monitoring before beginning iterations. Consistency in these steps ensures that the service levels are well defined and can be tracked effectively. Consider the following list as areas of focus when developing your internal process or SLO management infrastructure:

> * **SLI and SLO objectives and definitions**: Determined during the workshops, documented in a technical format, and prepared for consumption via automation.
> * **Automation**: The process we use for SLO change management to consume SLO objectives to make available for our platform of choice.
> * **Platform**: The platform we use to host and manage our SLOs and dashboards, which consumes SLO definitions through our automation to present to our audience.

In [*Chapter 11*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/11), we referenced observability platforms and SLO management platforms. Consider the automation focus as the process and tooling used to efficiently deploy your SLOs to your platform of choice. In some instances, an SLO management platform may not provide certain capabilities or calculate your SLOs in a way that is meaningful to your organization. You may then want to utilize internal platforms, such as an observability platform already integrated into systems you are using with internal metrics available to your dashboards. The automation step then becomes even more important to your process.

If we flow through the mentioned components of our SLO management infrastructure, the technical flow will consist of the following steps:

1. Document your SLI and SLO objective configuration files using the format appropriate for your tooling of choice.
2. Upload configuration files to a repository of choice.
3. Monitor the repository for changes to and deployment of supplied configuration files.
4. Apply configuration files via the API of your platform of choice.
5. Carry out CRUD operations for the management of SLO objects hosted in your platform of choice.

Each listed step is a high-level action that needs to be taken into consideration to better determine whether a specific internal platform and tool your team is already using integrates best with existing processes. In some cases, it may prove best to procure a platform that integrates each of the following. When building your SLO management infrastructure, it will serve you best to audit and take inventory of solutions your development teams are already utilizing internally.

In many instances, taking inventory of current code management practices will solve the problem if the right process is being used. However, in the event it does not, and you find your team navigating the current market for an SLO management platform, it remains best to understand the workflow to ensure the selected solutions serve your organization best. In addition, understanding the required flow and how it relates to your current workflows and processes will help your team to better adjust to naturally updating and adjusting SLIs and SLOs as needed and when appropriate in the life cycle.

# Adapting to SLI and SLO changes

A benefit of treating your SLO as code and creating an SLO management system is enabling your team and organization to naturally adapt and respond to SLI and SLO changes. We want to make sure we have processes in place, such as considering the level of observability into the systems being measured, that enable SRE to not only notice a system issue but also quickly determine whether there is an actual issue or a code or environment change that was not taken into consideration. SLIs and SLOs are implemented through metrics based on components within a system, which work together to provide your customers with specific functionality.

For instance, let’s return to earlier architectures mentioned in [*Chapter 7*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/7) of this book. In a multi-region cluster, we have several nodes hosting several services providing a UI for purchases to external customers. Several of the workloads consist of replicas to ensure high availability and efficient load balancing across regions. Our SRE team may have internal dashboards that monitor SLOs for specific features, such as checkout and payment processing functionality.

At the backend, the development team could push a change to the source code base, to incorporate a new method of payment. Our SLO for successful payment is being reported with a 98% success rate, and a large percentage of the user base is not using this method just yet. However, with the way the system metrics are calculated, the new payment method is not included in our data query and numerous failures are going unnoticed via SLO reporting but are being reported through the help desk and customer success channels. How do we as an SRE team implement processes that help to adapt to changes such as this? Adapting to SLI and SLO changes is a responsibility that will require adjusting over time.

Legacy systems may indeed experience fewer code base updates, but it’s important to note that customer feedback should not only inform new features but also be used to reassess and refine SLIs and SLOs. As an SRE team, it’s essential to continuously monitor and adjust these metrics in response to evolving customer needs and system behavior. When new features and functionality are integrated, or if your system experiences additional changes in a more agile environment, it will require the team to become more aware of how these changes impact established SLIs and SLOs. Occasionally, you may experience updates to the system, changes in user behaviors, or shifts in your business’s priorities, which will also require adjusting SLOs and response strategies as needed to align with changes in requirements and other circumstances.

Being adaptable and responsive to changes in the environment means adjusting SLOs and associated response strategies as needed to align with changing requirements and circumstances. Creating SLIs and SLOs will naturally help your team to measure your services in a way that is easy to align back to business operations and strategy through practices such as the following:

* AI-driven anomaly detection
* Automated incident response orchestration
* SLO-driven chaos engineering
* SLO-driven service dependency mapping
* Dynamic SLO adjustments

An important aspect of implementing SLIs and SLOs is to improve the customer experience by quantifying service performance in a way that benefits the customer. Even more, it provides internal teams with the ability to build internal metrics that provide insight into system performance to make informed decisions when issues and downtime occur. Apart from making informed decisions when issues and downtime occur, being able to swiftly and efficiently manage incidents is also important.

Although AI is not the topic of this chapter, or book, it’s an important reference to serve as a reminder when attempting to identify internal gaps for improvement. With the advances in the adoption of AI and its incorporation into various software and technology, it’s important to look for opportunities to bring it into your SLO management process and platform to improve automation, detection, and other areas that may have been overlooked.

Let’s read further and revisit ways each of the preceding practices can help to improve the daily operations of the SLO management process.

## AI-driven anomaly detection

In [*Chapter 11*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/11), when reviewing SLOs and walking through creating the SLO threshold and alert in your platform of choice, we made mention of anomalies within the CDN system due to data cache misses. This process focuses on leveraging the capabilities of machine learning algorithms and AI-powered systems to detect anomalies that could signal potential breaches of SLOs.

Current systems focus on rule-based algorithms on the backend to detect anomalies in behaviors of your systems, lacking manual adoption. With AI-driven detection, your team will focus on the following capabilities:

* **Data mining**: Data-gathering initiatives surrounding the collection of data emitted from internal hardware systems.
* **Data storage**: Establishing a storage solution that hosts data from data gathering. This solution will align with the type of workloads you are emitting data from.
* **Building AI models**: Models built by your team or fine-tuned from existing models, which incorporate linear regression, logistic regression, decision trees, and so on to recognize patterns and formulate a decision using relevant data presented.

Although this book does not focus on the power of AI or ML, it’s important to understand the evolving landscape of software and technology, while identifying areas of opportunity to incorporate such advancements. A common AI model reference is **Large Language** **Models** (**LLMs**).

## Automated incident response orchestration

Incident management is a pillar of its own within the reliability engineering hierarchy. It is also crucial to the success of a product when the product falls short of meeting the customer’s expectations. Within your incident management process, your team likely uses a severity and priority system for incidents in addition to maintaining internal SLAs for incident response times and RCA.

As a part of daily operations, and to reduce response times and engineer burnout, as well as achieving set SLAs, leverage opportunities to do the following:

* Respond swiftly to incidents via automated templated initial responses to incidents.
* Promptly identify and categorize incidents that impact any SLIs and SLOs.
* Quickly update and redirect incidents to their respective queues, if applicable.

Although alerting capabilities are typically built into the platform of choice, it’s important to remain proactive with the handling of incidents violating their SLO. In hindsight, merely reviewing dashboards every morning, looking for a topic to discuss, can easily become a tedious and redundant task. Within reliability engineering, SREs should naturally look to eliminate toil, when possible, to increase the amount of time available to innovate.

## SLO-driven chaos engineering

Chaos engineering consists of experimentation on a system to build confidence in the system and assess its ability to withstand chaos in a production environment (Chaos Community, 2019). Essentially, we want to break things to better understand specific behaviors and test a system’s capacity. A similar approach can and should be utilized to validate aspects of your system as it relates to SLO thresholds. If your team experiences unusual anomalies or spikes or SLO violations, use them to leverage and build “test cases” that challenge your system.

There are some new SRE platforms currently in development, such as the following:

* **Blameless**: Dedicated to providing solutions to SRE needs for reliably managing incidents through an incident management-based platform. It also includes an SLO manager to manage reliability objectives.
* **Steadybit**: Provides a tool for chaos engineering to test the reliability of your applications and infrastructure in cloud and on-premises environments.

These replicate some of the most common issues systems experience, while allowing the opportunity to customize test cases as needed.

## SLO-driven service dependency mapping

In a later section, we’ll discuss how service dependency mapping contributes toward working cross-functionally. However, it’s important to mention it here as a contributor toward successful SLO management. Dependency mapping focuses on the underlying relationships between your systems to better triage issues and understand the collective goal they are trying to accomplish. Outside of engaging with the product engineering teams developing the respective services, SLO-driven service dependency mapping helps SREs display service dependencies based on real-time or historical communication data, enabling a dynamic view of the interrelationships and performance bottlenecks within the system. This improves the ability of SREs to do the following:

* Display application traffic flow and relevant dependencies within a system at a given point in time.
* Quickly differentiate between applications reporting an issue and applications causing an issue.
* Get insights into the performance of distributed systems and applications through efficient event logging.

When including service dependency mapping within your SLO infrastructure, the team will have increased visibility into the relationships between each of the services when issues arise. For instance, during peak hours over the holiday season, the services associated with ordering a product are likely to experience an increase in sales and traffic. In the event of the system experiencing throttling, and either auto-scaling occurs or load balancing routes traffic to another region, when an issue arises, the service dependency chart will route the on-call engineer to the services directly involved in throttling. Most dependency charts will provide you with metadata related to the service, while highlighting errors or provide request and response network information to further debug.

## Dynamic SLO adjustments

In earlier workshop chapters, we made mention of an SLO to monitor system performance during the peak hours of sales. Dynamic SLO adjustment enables your platform of choice to tailor SLO targets dynamically based on changing circumstances. During those peak periods of high user demand or increased system load, it will service your on-call teams best to adjust SLO thresholds and prioritize service availability over response times. If your team is concerned about missing an incident, AI-driven anomaly detection can act as a filter to alleviate some of those concerns.

During critical incidents or service disruptions, it may be necessary to tighten SLO targets, ensuring swift resolution and minimal impact on the user experience. If an organization sells technical solutions with a business objective to provide a platform for developer enablement, it’s not ideal for a key enterprise account to experience a critical issue during an event, such as releasing a new application to consumers. Tightening any SLO targets surrounding the respective persona journey can lead to delivering good service to loyal customers.

# Cross-functional collaboration for SLIs and SLOs

When developing knowledge and experience of the SRE hierarchy and pillars within each layer, my primary experience was centered around container orchestration with vanilla and enterprise Kubernetes environments. In a microservice architecture for larger systems, we can automatically assume that there are several teams responsible for their respective service, associated integrations, and automation pipelines or processes used to deploy and manage changes.

As much as an all-hands-on-deck approach can appear to solve all our communication and collaboration issues, it can end up doing the opposite. The more teams and individuals responsible for various system components and reporting to different management chains, the more confusing it can be. This can also increase the chances of communication gaps, unclear ownership, and incorrect delegation of tasks. As mentioned in earlier chapters, how we communicate and work across teams to reduce silos and increase efficient communication greatly impacts the success of implementation and speed of adoption.

In larger organizations, you will experience an increase in the number of teams and various tooling and processes used within each team if there is a lack of unification and process streamlining. As it relates to the SLI and SLO process, you want to consider the following to foster a cross-functional communicative culture:

* Service dependency mapping
* The incident management structure
* Cross-functional touchpoints
* The offboarding process

Each of these are things you want to take into consideration during the beginning stages to improve the collaborative aspects of your SLO management process and increase the buy-in of SLOs within the broader organization. The introduction and expectations of each at the start of the conversation will contribute a great deal to the success of your SLO journey.

## Service dependency mapping

Service dependency mapping will help your team navigate the system and triage incidents by identifying the root cause and understanding the impact on dependent services. In addition, during an active incident, on-call engineers are more likely to quickly engage with the appropriate team, in the event alerting and triaging are out of alignment.

When procuring an SRE platform, especially for Incident Management or Observability focused platforms, ensure service dependency mapping is a highlighted feature. On-call alerting platforms, such as PagerDuty, also provide the capability to structure alert routing in a similar manner.

## The incident management structure

Incident management is discussed further in [*Chapter 13*](https://subscription.packtpub.com/book/cloud-and-networking/9781835889381/13). It’s the system of processes used to manage an incident’s life cycle. The process you use for responding to SLO violations should replicate or at least be like that of the teams handling incident management for the respective services. SRE platforms for incident management, such as Blameless, mentioned earlier in this chapter, will combine incident management and SLO management capabilities, incorporating them into the platform, unifying the process and tooling for on-call engineers responding to incidents for different services or in different on-call rotations, but requiring a similar process and system access. Unifying incident management will improve a few areas for you and your team(s):

* Avoid reinventing the wheel and adding another process to an engineering (or on-call) team that likely already has many processes.

* It enables your team to bring value to the incident management system through process improvement. If you are in an organization where incident management is simply a bunch of alerts firing, it’s an opportunity to offer other SRE services and revamp the incident management structure.

* It can improve response times. On-call teams are likely already busy. Understanding how the team runs the incident management life cycle, structure, and unifying incident response will associate your process with a process they are already familiar with – which, if operating on muscle memory, will make it easier for on-call teams to respond immediately to you.

## Cross-functional touchpoints

Cross-functional touchpoints include creating a cadence to interact with the teams responsible for developing the services you are implementing SLOs for. As things change within the system, it’s important to create an avenue for communication in a healthy way. The more development teams understand what you are doing and why, the easier it will be for them to adapt. There are also other benefits, such as relationship building, which helps greatly when something is needed.

Otherwise, it’s likely that your team will have specific meeting cadences with stakeholders. Just don’t forget about the folks who know and engineer your systems. This can include the following:

* Establishing a healthy cadence for internal meetings with development or engineering teams and management will help to keep your team on the right track. Take into consideration the schedule for meetings relating to internal teams completing the SLO work.
* Quarterly meetings incorporating management, stakeholders, and other necessary executives will ensure SRE is in sync with the strategy of the broader organization.

## Offboarding process

If your SRE team or the team initially managing the SLO process is an enablement or initialization team, you’ll want to ensure you have an effective offboarding process. This may even include providing enablement documentation or flowing through and documenting the SLO process in a way that is consumable later.

It’s important not to consider the process as simply handing over an SLO dashboard to another team. Instead, it should include ensuring proper knowledge transfer of the technical tooling used to manage SLOs as code, along with the configuration of alerting mechanisms and monitoring practices. This has been highlighted as the more transparency and understanding of the process, the easier it will be for another team to adopt the process and SLO process. Otherwise, it can end up being a dashboard that no one pays attention to after a while.

# SLI and SLO performance reviews

An additional objective of your SLIs and SLOs is to establish a system of internal accountability, encouraging collaboration between teams responsible for engineering the system components and those managing customer-facing interactions that affect service reliability. This will require each team to naturally work in a cross-functional manner. However, we are also aware that there are only so many hours in a workday where we can consistently obtain the necessary individuals for the same meeting. Working in a virtual environment only makes this a more difficult feat, requiring a strategy to ensure the individuals have the appropriate face-to-face conversations to ensure that SLIs and SLOs remain as follows:

* Performant at the level necessary for business operations.
* Relevant to the target persona and the original problem identified.
* Necessary. Even if the SLIs and SLOs are relevant to a problem, is the problem still relevant to the original stakeholder?
* Impactful to business operations through decision-making between key stakeholders and team members.

Improving the sustainability of your SLOs and their management process relies heavily on consistent cross-functional touchpoints. These touchpoints often prove more impactful than hierarchal updates for driving collaboration and maintaining alignment between teams. You have a great chance of improving adoption rates when working cross-functionally with individuals handling daily operations. We can achieve the mentioned goals through lateral touchpoints and overarching SLO performance reviews. Some things you may want to consider are the following:

* Access to tooling related to documentation
* Access related to automated tooling
* Access related to cloud provider platforms
* Access related to SSO-related product lines

Although this is not a topic with its own chapter in this book, it’s important to be aware of the security-related aspects of SLO management. This includes access that relates to internal and external tooling supporting your SLOs. Your SLOs are essentially reporting the performance of the services critical to your business and should be treated as important. It’s best to incorporate topics such as this during your SLO workshops as well.

# Summary

A proactive and strategic approach is required for adopting SLIs and SLOs to measure the performance of your systems and services and reliably address changing business objectives, market conditions, and technological advancements. Regardless of the methodology and tooling you choose, each should be a part of a larger process that enables assessment of the alignment between SLIs, SLOs, and business goals. It’s equally important to ensure there exists the ability to adjust metrics and targets accordingly. When both exist as a part of your process, your organization can confidently meet external-facing commitments, such as SLAs for high-performing services.

Implementing dynamic practices will help ensure you are consistently reliable in a way that is scalable to your organization. It will also improve your ability to remain responsive to issues over time, with SLOs at the core. Meeting user expectations and driving business success over the long term depends heavily on our ability to maintain relevant practices for effective SLO maintenance.

Monitoring SLIs in real time and responding to SLIs and SLOs through automation will improve your ability to ensure alignment between business objectives and reduce human error. You will also notice an improved ability to remain agile, adaptive, and resilient in the face of responding to new feature requests and responding to customer issues and inquiries. Through continuous iteration, improvement, and alignment with user needs and the continuous changes in market dynamics, you will uphold service reliability and deliver exceptional experiences to internal and external users and customers.

In the next chapter, we’ll discuss the importance of integrating internal incident management processes to efficiently manage incident metrics, resulting in improved SLO preservation capabilities.

# Further reading

Refer to the following for additional reading about concepts mentioned in this chapter:

* Chaos Community. (2019, March). *Principles of Chaos* *Engineering*: <https://principlesofchaos.org>.
